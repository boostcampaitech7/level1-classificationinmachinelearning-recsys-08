{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ ì¤‘ì—ì„œ ì•ˆê¹”ë¦° ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ìˆì„ ìˆ˜ ìˆìŒ\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import lightgbm as lgb\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../data/\"\n",
    "\n",
    "train = pd.read_csv(path+\"train.csv\").assign(_type=\"train\")\n",
    "test = pd.read_csv(path+\"test.csv\").assign(_type=\"test\")\n",
    "submission = pd.read_csv(path+\"test.csv\")\n",
    "df = pd.concat([train, test], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ì›ë˜ ë² ì´ìŠ¤ë¼ì¸ ì½”ë“œ ë¶€ë¶„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # HOURLY_ ë¡œ ì‹œì‘í•˜ëŠ” .csv íŒŒì¼ ì´ë¦„ì„ file_names ì— í• ë”©\n",
    "# file_names: List[str] = [\n",
    "#     f for f in os.listdir(data_path) if f.startswith(\"HOURLY_\") and f.endswith(\".csv\")\n",
    "# ]\n",
    "\n",
    "# # íŒŒì¼ëª… : ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì €ì¥\n",
    "# file_dict: Dict[str, pd.DataFrame] = {\n",
    "#     f.replace(\".csv\", \"\"): pd.read_csv(os.path.join(data_path, f)) for f in file_names\n",
    "# }\n",
    "\n",
    "# for _file_name, _df in tqdm(file_dict.items()):\n",
    "#     # ì—´ ì´ë¦„ ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ {_file_name.lower()}_{col.lower()}ë¡œ ë³€ê²½, datetime ì—´ì„ IDë¡œ ë³€ê²½\n",
    "#     _rename_rule = {\n",
    "#         col: f\"{_file_name.lower()}_{col.lower()}\" if col != \"datetime\" else \"ID\"\n",
    "#         for col in _df.columns\n",
    "#     }\n",
    "#     _df = _df.rename(_rename_rule, axis=1)\n",
    "#     df = df.merge(_df, on=\"ID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ë°‘ì˜ í•œ ì…€ì´ ë°”ê¾¼ ë¶€ë¶„ (ì´ ë°”ê¾¼ ë¶€ë¶„ì„ ì „ì œë¡œ ë’¤ì˜ ì½”ë“œê°€ ì‹¤í–‰ë¨.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HOURLY_NETWORK-DATA_SUPPLY.csv',\n",
       " 'HOURLY_MARKET-DATA_COINBASE-PREMIUM-INDEX.csv',\n",
       " 'HOURLY_NETWORK-DATA_DIFFICULTY.csv',\n",
       " 'HOURLY_NETWORK-DATA_ADDRESSES-COUNT.csv',\n",
       " 'HOURLY_NETWORK-DATA_UTXO-COUNT.csv',\n",
       " 'HOURLY_NETWORK-DATA_VELOCITY.csv',\n",
       " 'HOURLY_MARKET-DATA_LIQUIDATIONS_ALL_EXCHANGE_ALL_SYMBOL.csv',\n",
       " 'HOURLY_NETWORK-DATA_FEES.csv',\n",
       " 'HOURLY_NETWORK-DATA_HASHRATE.csv',\n",
       " 'HOURLY_MARKET-DATA_FUNDING-RATES_ALL_EXCHANGE.csv',\n",
       " 'HOURLY_NETWORK-DATA_BLOCKREWARD.csv',\n",
       " 'HOURLY_NETWORK-DATA_FEES-TRANSACTION.csv',\n",
       " 'HOURLY_NETWORK-DATA_TRANSACTIONS-COUNT.csv',\n",
       " 'HOURLY_MARKET-DATA_OPEN-INTEREST_ALL_EXCHANGE_ALL_SYMBOL.csv',\n",
       " 'HOURLY_NETWORK-DATA_BLOCK-BYTES.csv',\n",
       " 'HOURLY_NETWORK-DATA_TOKENS-TRANSFERRED.csv',\n",
       " 'HOURLY_MARKET-DATA_PRICE-OHLCV_ALL_EXCHANGE_SPOT_BTC_USD.csv',\n",
       " 'HOURLY_NETWORK-DATA_BLOCK-INTERVAL.csv',\n",
       " 'HOURLY_NETWORK-DATA_BLOCK-COUNT.csv',\n",
       " 'HOURLY_MARKET-DATA_TAKER-BUY-SELL-STATS_ALL_EXCHANGE.csv']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 30.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# HOURLY_ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  csv íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "prefixes = [\"HOURLY_NETWORK-DATA_\",\n",
    "            \"HOURLY_MARKET-DATA_PRICE-OHLCV_ALL_EXCHANGE_SPOT_BTC_USD\",\n",
    "            \"HOURLY_MARKET-DATA_COINBASE-PREMIUM-INDEX\",\n",
    "            \"HOURLY_MARKET-DATA_FUNDING-RATES_ALL_EXCHANGE\",\n",
    "            \"HOURLY_MARKET-DATA_LIQUIDATIONS_ALL_EXCHANGE_ALL_SYMBOL\",\n",
    "            \"HOURLY_MARKET-DATA_OPEN-INTEREST_ALL_EXCHANGE_ALL_SYMBOL\",\n",
    "            \"HOURLY_MARKET-DATA_TAKER-BUY-SELL-STATS_ALL_EXCHANGE\"]\n",
    "\n",
    "file_names = [\n",
    "    f for f in os.listdir(path)\n",
    "    if any(f.startswith(prefix) for prefix in prefixes) and f.endswith(\".csv\")\n",
    "]\n",
    "display(file_names)\n",
    "file_dict: Dict[str, pd.DataFrame] = {\n",
    "    f.replace(\".csv\", \"\"): pd.read_csv(path+f) for f in file_names\n",
    "}\n",
    "\n",
    "\n",
    "for _file_name, _df in tqdm(file_dict.items()):\n",
    "    _rename_rule = {\n",
    "        col: f\"{col.lower()}\" if col != \"datetime\" else \"ID\"\n",
    "        for col in _df.columns\n",
    "    }\n",
    "    _df = _df.rename(_rename_rule, axis=1)\n",
    "    df = df.merge(_df, on=\"ID\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ê²°ì¸¡ì¹˜ (ì•„ê±°ë„ ì¶”ê°€ëœ ë¶€ë¶„)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "volume                         2792\n",
       "close                          2792\n",
       "block_bytes                      54\n",
       "funding_rates                    32\n",
       "taker_buy_sell_ratio             31\n",
       "taker_sell_ratio                 31\n",
       "taker_buy_ratio                  31\n",
       "taker_sell_volume                31\n",
       "taker_buy_volume                 31\n",
       "transactions_count_mean          28\n",
       "supply_new                       28\n",
       "supply_total                     28\n",
       "fees_transaction_mean_usd        24\n",
       "fees_block_mean                  24\n",
       "fees_block_mean_usd              24\n",
       "fees_reward_percent              24\n",
       "difficulty                       24\n",
       "tokens_transferred_mean          24\n",
       "block_interval                   24\n",
       "fees_transaction_mean            24\n",
       "coinbase_premium_gap              6\n",
       "coinbase_premium_index            6\n",
       "transactions_count_total          4\n",
       "open_interest                     4\n",
       "block_count                       4\n",
       "utxo_count                        1\n",
       "addresses_count_sender            0\n",
       "addresses_count_receiver          0\n",
       "tokens_transferred_median         0\n",
       "tokens_transferred_total          0\n",
       "addresses_count_active            0\n",
       "long_liquidations_usd             0\n",
       "velocity_supply_total             0\n",
       "short_liquidations_usd            0\n",
       "fees_transaction_median_usd       0\n",
       "fees_transaction_median           0\n",
       "blockreward_usd                   0\n",
       "long_liquidations                 0\n",
       "short_liquidations                0\n",
       "hashrate                          0\n",
       "fees_total_usd                    0\n",
       "fees_total                        0\n",
       "blockreward                       0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "volume                         0.241690\n",
       "close                          0.241690\n",
       "block_bytes                    0.004675\n",
       "funding_rates                  0.002770\n",
       "taker_buy_sell_ratio           0.002684\n",
       "taker_sell_ratio               0.002684\n",
       "taker_buy_ratio                0.002684\n",
       "taker_sell_volume              0.002684\n",
       "taker_buy_volume               0.002684\n",
       "transactions_count_mean        0.002424\n",
       "supply_new                     0.002424\n",
       "supply_total                   0.002424\n",
       "fees_transaction_mean_usd      0.002078\n",
       "fees_block_mean                0.002078\n",
       "fees_block_mean_usd            0.002078\n",
       "fees_reward_percent            0.002078\n",
       "difficulty                     0.002078\n",
       "tokens_transferred_mean        0.002078\n",
       "block_interval                 0.002078\n",
       "fees_transaction_mean          0.002078\n",
       "coinbase_premium_gap           0.000519\n",
       "coinbase_premium_index         0.000519\n",
       "transactions_count_total       0.000346\n",
       "open_interest                  0.000346\n",
       "block_count                    0.000346\n",
       "utxo_count                     0.000087\n",
       "addresses_count_sender         0.000000\n",
       "addresses_count_receiver       0.000000\n",
       "tokens_transferred_median      0.000000\n",
       "tokens_transferred_total       0.000000\n",
       "addresses_count_active         0.000000\n",
       "long_liquidations_usd          0.000000\n",
       "velocity_supply_total          0.000000\n",
       "short_liquidations_usd         0.000000\n",
       "fees_transaction_median_usd    0.000000\n",
       "fees_transaction_median        0.000000\n",
       "blockreward_usd                0.000000\n",
       "long_liquidations              0.000000\n",
       "short_liquidations             0.000000\n",
       "hashrate                       0.000000\n",
       "fees_total_usd                 0.000000\n",
       "fees_total                     0.000000\n",
       "blockreward                    0.000000\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_new = df.drop([\"ID\", \"target\", \"_type\"], axis = 1)\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ í™•ì¸\n",
    "missing_data = df_new.isnull().sum()\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ ë¹„ìœ¨ í™•ì¸\n",
    "missing_ratio = df_new.isnull().mean()\n",
    "\n",
    "\n",
    "display(missing_data.sort_values(ascending=False))\n",
    "display(missing_ratio.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supply_total                   0\n",
      "supply_new                     0\n",
      "coinbase_premium_gap           0\n",
      "coinbase_premium_index         0\n",
      "difficulty                     0\n",
      "addresses_count_active         0\n",
      "addresses_count_sender         0\n",
      "addresses_count_receiver       0\n",
      "utxo_count                     0\n",
      "velocity_supply_total          0\n",
      "long_liquidations              0\n",
      "short_liquidations             0\n",
      "long_liquidations_usd          0\n",
      "short_liquidations_usd         0\n",
      "fees_block_mean                0\n",
      "fees_block_mean_usd            0\n",
      "fees_total                     0\n",
      "fees_total_usd                 0\n",
      "fees_reward_percent            0\n",
      "hashrate                       0\n",
      "funding_rates                  0\n",
      "blockreward                    0\n",
      "blockreward_usd                0\n",
      "fees_transaction_mean          0\n",
      "fees_transaction_mean_usd      0\n",
      "fees_transaction_median        0\n",
      "fees_transaction_median_usd    0\n",
      "transactions_count_total       0\n",
      "transactions_count_mean        0\n",
      "open_interest                  0\n",
      "block_bytes                    0\n",
      "tokens_transferred_total       0\n",
      "tokens_transferred_mean        0\n",
      "tokens_transferred_median      0\n",
      "close                          0\n",
      "volume                         0\n",
      "block_interval                 0\n",
      "block_count                    0\n",
      "taker_buy_volume               0\n",
      "taker_sell_volume              0\n",
      "taker_buy_ratio                0\n",
      "taker_sell_ratio               0\n",
      "taker_buy_sell_ratio           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. ê²°ì¸¡ì¹˜ ì²˜ë¦¬ - ì•/ë’¤ ë³´ê°„ë²• (forward/backward fill)\n",
    "\n",
    "# df_ffill = df_new.fillna(method='ffill')  # ì•ì˜ ê°’ì„ ì‚¬ìš©í•˜ì—¬ ê²°ì¸¡ì¹˜ ì±„ì›€\n",
    "# df_bfill = df_new.fillna(method='bfill')  # ë’¤ì˜ ê°’ì„ ì‚¬ìš©í•˜ì—¬ ê²°ì¸¡ì¹˜ ì±„ì›€\n",
    "\n",
    "# 2. ê²°ì¸¡ì¹˜ ì²˜ë¦¬ - ì„ í˜• ë³´ê°„ë²• (linear interpolation)\n",
    "df_interpolated = df_new.interpolate(method='linear')\n",
    "\n",
    "# 3. ê²°ì¸¡ì¹˜ ì²˜ë¦¬ - í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´\n",
    "# df_mean_filled = df_new.fillna(eda_df.mean())\n",
    "\n",
    "df.update(df_interpolated)\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í›„ í™•ì¸\n",
    "print(df_interpolated.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë§Œì•½ ë°ì´í„°ë¥¼ ë¡œë“œí•  ë•Œ, (ì œê³µ ë°›ì€)ë² ì´ìŠ¤ë¼ì¸ ì½”ë“œì—ì„œì™€ ê°™ì´ ê±°ë˜ì†Œì™€ ì•”í˜¸í™”í ì¢…ë¥˜ë¥¼ ë¶ˆëŸ¬ì˜¤ë„ë¡ ì„¤ì •ì„ í–ˆë‹¤ë©´\n",
    "\n",
    "ì•„ë˜ ì½”ë“œëŠ” ì‘ë™í•˜ì§€ ì•ŠìŒ\n",
    "\n",
    "ê±°ë˜ì†Œì™€ ì•”í˜¸í™”í ì¢…ë¥˜ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šì€ ì½”ë“œì„ì„ ìœ ì˜.\n",
    "\n",
    "(ìˆ˜ì •ì„ í•´ì•¼í•  ê²ƒ ê°™ìŒ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²´ ë…ë¦½ë³€ìˆ˜ì—ì„œ ë‚´ê°€ ì‚¬ìš©í•  ë…ë¦½ë³€ìˆ˜ ê³ ë¥´ê¸°\n",
    "\n",
    "\n",
    "# ì „ì²´ feature\n",
    "features = df.columns\n",
    "# ë‚´ê°€ ì“¸ feature\n",
    "fe = ['ID', 'target', '_type', 'funding_rates', 'open_interest',\n",
    "       'taker_buy_volume', 'taker_sell_volume', 'taker_buy_ratio',\n",
    "       'taker_sell_ratio', 'taker_buy_sell_ratio', 'difficulty',\n",
    "       'transactions_count_total', 'transactions_count_mean', 'block_count',\n",
    "       'fees_transaction_mean', 'fees_transaction_mean_usd',\n",
    "       'fees_transaction_median', 'fees_transaction_median_usd',\n",
    "       'fees_block_mean', 'fees_block_mean_usd', 'fees_total',\n",
    "       'fees_total_usd', 'fees_reward_percent', 'hashrate', 'utxo_count',\n",
    "       'tokens_transferred_total', 'tokens_transferred_mean',\n",
    "       'tokens_transferred_median', 'block_interval', 'velocity_supply_total',\n",
    "       'supply_total', 'supply_new', 'addresses_count_active',\n",
    "       'addresses_count_sender', 'addresses_count_receiver',\n",
    "       'blockreward', 'blockreward_usd', 'buy_sell_volume_ratio']\n",
    "\n",
    "# new_featuresì—ëŠ” ë‚´ê°€ ì‚¬ìš©í•˜ì§€ ì•Šì„ featureë“¤ì´ ë‹´ê¸°ê³  ë‚˜ì¤‘ì— dropë¨. ë•Œë¬¸ì—, ì´ë¦„ì´ ì¢€ ì˜ëª»ëœ ë“¯\n",
    "new_features = [x for x in features if x not in fe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda ì—ì„œ íŒŒì•…í•œ ì°¨ì´ì™€ ì°¨ì´ì˜ ìŒìˆ˜, ì–‘ìˆ˜ ì—¬ë¶€ë¥¼ ìƒˆë¡œìš´ í”¼ì³ë¡œ ìƒì„±\n",
    "df = df.assign(\n",
    "    liquidation_diff=df[\"long_liquidations\"] - df[\"short_liquidations\"],\n",
    "    liquidation_usd_diff=df[\"long_liquidations_usd\"] - df[\"short_liquidations_usd\"],\n",
    "    volume_diff=df[\"taker_buy_volume\"] - df[\"taker_sell_volume\"],\n",
    "    liquidation_diffg=np.sign(df[\"long_liquidations\"] - df[\"short_liquidations\"]),\n",
    "    liquidation_usd_diffg=np.sign(df[\"long_liquidations_usd\"] - df[\"short_liquidations_usd\"]),\n",
    "    volume_diffg=np.sign(df[\"taker_buy_volume\"] - df[\"taker_sell_volume\"]),\n",
    "    buy_sell_volume_ratio=df[\"taker_buy_volume\"] / (df[\"taker_sell_volume\"] + 1),\n",
    ")\n",
    "\n",
    "# category, continuous ì—´ì„ ë”°ë¡œ í• ë‹¹í•´ë‘ \n",
    "category_cols = [\"liquidation_diffg\", \"liquidation_usd_diffg\", \"volume_diffg\"]\n",
    "conti_cols = [_ for _ in (df.columns)[3:] if _ not in [\"ID\", \"target\", \"_type\"]] + [\n",
    "    \"buy_sell_volume_ratio\",\n",
    "    \"liquidation_diff\",\n",
    "    \"liquidation_usd_diff\",\n",
    "    \"volume_diff\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_feature(\n",
    "    df: pd.DataFrame,\n",
    "    conti_cols: List[str],\n",
    "    intervals: List[int],\n",
    ") -> List[pd.Series]:\n",
    "    \"\"\"\n",
    "    ì—°ì†í˜• ë³€ìˆ˜ì˜ shift feature ìƒì„±\n",
    "    Args:\n",
    "        df (pd.DataFrame)\n",
    "        conti_cols (List[str]): continuous colnames\n",
    "        intervals (List[int]): shifted intervals\n",
    "    Return:\n",
    "        List[pd.Series]\n",
    "    \"\"\"\n",
    "    df_shift_dict = [\n",
    "        df[conti_col].shift(interval).rename(f\"{conti_col}_{interval}\")\n",
    "        for conti_col in conti_cols\n",
    "        for interval in intervals\n",
    "    ]\n",
    "    return df_shift_dict\n",
    "\n",
    "# ì§€ìˆ˜ ì´ë™ í‰ê· \n",
    "def EMA(df, col, span=2):\n",
    "    return df[col].ewm(span=span).mean()\n",
    "\n",
    "# Wavlet Transform\n",
    "def WT(df, col, wavelet='db5', th=0.6):\n",
    "    signal = df[col].values\n",
    "    th = th*np.nanmax(signal)\n",
    "    coef = pywt.wavedec(signal, wavelet, mode=\"per\" )\n",
    "    coef[1:] = (pywt.threshold(i, value=th, mode=\"soft\" ) for i in coef[1:])\n",
    "    reconstructed = pywt.waverec(coef, wavelet, mode=\"per\" )\n",
    "    return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ğŸ”¥ í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§ í•¨ìˆ˜ ì¶”ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‚ ì§œ ê´€ë ¨ ìƒì„±\n",
    "def make_date_features(df: pd.DataFrame, date_column: str) -> Tuple[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    ì…ë ¥ëœ ë°ì´í„°í”„ë ˆì„ì˜ íŠ¹ì • ë‚ ì§œ ì—´ì„ ê¸°ì¤€ìœ¼ë¡œ ì—°ë„, ì›”, ì£¼, ìš”ì¼, ì‹œê°„ì„ ì¶”ì¶œí•˜ì—¬ ìƒˆë¡œìš´ í”¼ì²˜ë¡œ ì¶”ê°€.\n",
    "    Args:\n",
    "        df (pd.DataFrame): ë‚ ì§œ ì»¬ëŸ¼ì„ í¬í•¨í•˜ëŠ” ë°ì´í„°í”„ë ˆì„.\n",
    "        date_column (str): ë‚ ì§œ ì •ë³´ê°€ ë‹´ê¸´ ì—´ ì´ë¦„.\n",
    "    Returns:\n",
    "        pd.DataFrame: ë‚ ì§œ ê´€ë ¨ í”¼ì²˜ê°€ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„ ë°˜í™˜.\n",
    "    \"\"\"\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "    df['year'] = df[date_column].dt.year  # ì—°ë„\n",
    "    df['month'] = df[date_column].dt.month  # ì›” \n",
    "    df['week'] = df[date_column].dt.isocalendar().week  # ì£¼\n",
    "    df['day_of_week'] = df[date_column].dt.dayofweek  # ìš”ì¼ \n",
    "    df['hour'] = df[date_column].dt.hour  # ì‹œê°„ \n",
    "    date_columns = [date_column, 'year', 'month', 'week', 'day_of_week', 'hour']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë³€ë™ì„±, ì°¨ë¶„ í”¼ì²˜ ìƒì„± í•¨ìˆ˜ \n",
    "def make_diff_change_feature(\n",
    "    df: pd.DataFrame, \n",
    "    columns_list: List[str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ ë³€ìˆ˜ë“¤ì— ëŒ€í•œ ë³€ë™ì„±, ì°¨ë¶„ í”¼ì²˜ë¥¼ ìƒì„±í•˜ê³ , ê²°ì¸¡ê°’ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜.\n",
    "    Args:\n",
    "        df (pd.DataFrame): ë°ì´í„°ë¥¼ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„.\n",
    "        columns_list (List[str]): ë³€ë™ì„±ê³¼ ì°¨ë¶„ í”¼ì²˜ë¥¼ ì¶”ê°€í•˜ê³ ì í•˜ëŠ” ì—´ ì´ë¦„ ëª©ë¡.\n",
    "    Returns:\n",
    "        pd.DataFrame: ë³€ë™ì„±ê³¼ ì°¨ë¶„ í”¼ì²˜ê°€ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„.\n",
    "    \"\"\"  \n",
    "    new_features = {}  # ìƒˆë¡œ ì¶”ê°€í•  ì—´ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬\n",
    "    for col in columns_list:\n",
    "        if col in df.columns:\n",
    "            pct_change_col = f'{col}_pct_change'\n",
    "            diff_col = f'{col}_diff'\n",
    "            new_features[pct_change_col] = df[col].pct_change(fill_method=None)\n",
    "            new_features[diff_col] = df[col].diff()\n",
    "        else:\n",
    "            print(f\"Error: Cannot find '{col}' column\")\n",
    "    new_features_df = pd.DataFrame(new_features, index=df.index) # ìƒˆ í”¼ì²˜ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ\n",
    "    new_features_df = new_features_df.ffill().bfill()  # ê²°ì¸¡ê°’ ì²˜ë¦¬ (ê³ ì •: ffill, bfill)\n",
    "    df = pd.concat([df, new_features_df], axis=1)  # ê¸°ì¡´ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¡±/ìˆ ë¹„ìœ¨\n",
    "def make_longshort_ratio_feature(\n",
    "    df: pd.DataFrame, \n",
    "    long_col: str, \n",
    "    short_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ë¡±/ìˆ ë¹„ìœ¨ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜\n",
    "    Args:\n",
    "        df (pd.DataFrame): ì…ë ¥ ë°ì´í„°í”„ë ˆì„\n",
    "        long_col (str): ë¡±(liquidations) ë°ì´í„°ê°€ ì €ì¥ëœ ì—´ ì´ë¦„\n",
    "        short_col (str): ìˆ(liquidations) ë°ì´í„°ê°€ ì €ì¥ëœ ì—´ ì´ë¦„\n",
    "    Returns:\n",
    "        pd.DataFrame: ë¡±/ìˆ ë¹„ìœ¨ì´ ì €ì¥ëœ ìƒˆë¡œìš´ ì—´ì´ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„\n",
    "    \"\"\"\n",
    "    df['long_short_ratio'] = df[long_col] / (df[short_col] + 1e-6)  # ë¶„ëª¨ê°€ 0ì´ ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì‘ì€ ê°’ì„ ë”í•¨\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²­ì‚°/ê±°ë˜ëŸ‰ ë¹„ìœ¨\n",
    "def make_liquidation_to_volume_ratio_feature(\n",
    "    df: pd.DataFrame, \n",
    "    long_col: str, \n",
    "    short_col: str, \n",
    "    buy_volume_col: str, \n",
    "    sell_volume_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ì²­ì‚°/ê±°ë˜ëŸ‰ ë¹„ìœ¨ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜\n",
    "    Args:\n",
    "        df (pd.DataFrame): ì…ë ¥ ë°ì´í„°í”„ë ˆì„\n",
    "        long_col (str): ë¡±(liquidations) ë°ì´í„°ê°€ ì €ì¥ëœ ì—´ ì´ë¦„\n",
    "        short_col (str): ìˆ(liquidations) ë°ì´í„°ê°€ ì €ì¥ëœ ì—´ ì´ë¦„\n",
    "        buy_volume_col (str): ë§¤ìˆ˜ ê±°ë˜ëŸ‰ì´ ì €ì¥ëœ ì—´ ì´ë¦„\n",
    "        sell_volume_col (str): ë§¤ë„ ê±°ë˜ëŸ‰ì´ ì €ì¥ëœ ì—´ ì´ë¦„\n",
    "    Returns:\n",
    "        pd.DataFrame: ì²­ì‚°/ê±°ë˜ëŸ‰ ë¹„ìœ¨ì„ ì €ì¥í•œ ìƒˆë¡œìš´ ì—´ì´ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„\n",
    "    \"\"\"\n",
    "    df['liquidation_to_volume_ratio'] = (\n",
    "        (df[long_col] + df[short_col]) / \n",
    "        (df[buy_volume_col] + df[sell_volume_col] + 1e-6)  # ë¶„ëª¨ê°€ 0ì´ ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì‘ì€ ê°’ì„ ë”í•¨\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²­ì‚°ëœ USD ë¡±/ìˆ ë¹„ìœ¨\n",
    "def make_liquidation_usd_ratio_feature(\n",
    "    df: pd.DataFrame, \n",
    "    long_usd_col: str, \n",
    "    short_usd_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ì²­ì‚°ëœ USD ë¡±/ìˆ ë¹„ìœ¨ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜\n",
    "    Args:\n",
    "        df (pd.DataFrame): ì…ë ¥ ë°ì´í„°í”„ë ˆì„\n",
    "        long_usd_col (str): ë¡±(liquidations) USD ë°ì´í„°ê°€ ì €ì¥ëœ ì—´ ì´ë¦„\n",
    "        short_usd_col (str): ìˆ(liquidations) USD ë°ì´í„°ê°€ ì €ì¥ëœ ì—´ ì´ë¦„ \n",
    "    Returns:\n",
    "        pd.DataFrame: ì²­ì‚°ëœ USD ë¡±/ìˆ ë¹„ìœ¨ì„ ì €ì¥í•œ ìƒˆë¡œìš´ ì—´ì´ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„\n",
    "    \"\"\"\n",
    "    df['liquidation_usd_ratio'] = df[long_usd_col] / (df[short_usd_col] + 1e-6)  # ë¶„ëª¨ê°€ 0ì´ ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì‘ì€ ê°’ì„ ë”í•¨\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í€ë”© ë¹„ìœ¨ê³¼ ë¡±/ìˆ í¬ì§€ì…˜ ì°¨ì´ ê³±\n",
    "def make_funding_rate_position_change_feature(\n",
    "    df: pd.DataFrame, \n",
    "    funding_rate_col: str, \n",
    "    long_liquidations_col: str, \n",
    "    short_liquidations_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    í€ë”© ë¹„ìœ¨ê³¼ ë¡±/ìˆ í¬ì§€ì…˜ ì°¨ì´ë¥¼ ê³±í•˜ì—¬ í¬ì§€ì…˜ ë³€í™”ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜\n",
    "    Args:\n",
    "        df (pd.DataFrame): ì…ë ¥ ë°ì´í„°í”„ë ˆì„\n",
    "        funding_rate_col (str): í€ë”© ë¹„ìœ¨ ë°ì´í„°ê°€ ì €ì¥ëœ ì—´ ì´ë¦„\n",
    "        long_liquidations_col (str): ë¡± ì²­ì‚° ë°ì´í„°ê°€ ì €ì¥ëœ ì—´ ì´ë¦„\n",
    "        short_liquidations_col (str): ìˆ ì²­ì‚° ë°ì´í„°ê°€ ì €ì¥ëœ ì—´ ì´ë¦„\n",
    "    Returns:\n",
    "        pd.DataFrame: í€ë”© ë¹„ìœ¨ì— ë”°ë¥¸ í¬ì§€ì…˜ ë³€í™”ë¥¼ ì €ì¥í•œ ìƒˆë¡œìš´ ì—´ì´ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„\n",
    "    \"\"\"\n",
    "    df['funding_rate_position_change'] = df[funding_rate_col] * (\n",
    "        df[long_liquidations_col] - df[short_liquidations_col]\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”„ë¦¬ë¯¸ì—„ ê°­ê³¼ í”„ë¦¬ë¯¸ì—„ ì¸ë±ìŠ¤ì˜ ì°¨ì´\n",
    "def make_premium_diff_feature(\n",
    "    df: pd.DataFrame, \n",
    "    premium_gap_col: str, \n",
    "    premium_index_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    í”„ë¦¬ë¯¸ì—„ ê°­ê³¼ í”„ë¦¬ë¯¸ì—„ ì¸ë±ìŠ¤ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜\n",
    "    Args:\n",
    "        df (pd.DataFrame): ì…ë ¥ ë°ì´í„°í”„ë ˆì„\n",
    "        premium_gap_col (str): í”„ë¦¬ë¯¸ì—„ ê°­ ë°ì´í„°ê°€ ì €ì¥ëœ ì—´ ì´ë¦„\n",
    "        premium_index_col (str): í”„ë¦¬ë¯¸ì—„ ì¸ë±ìŠ¤ ë°ì´í„°ê°€ ì €ì¥ëœ ì—´ ì´ë¦„\n",
    "    Returns:\n",
    "        pd.DataFrame: í”„ë¦¬ë¯¸ì—„ ê°­ê³¼ í”„ë¦¬ë¯¸ì—„ ì¸ë±ìŠ¤ì˜ ì°¨ì´ë¥¼ ì €ì¥í•œ ìƒˆë¡œìš´ ì—´ì´ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„\n",
    "    \"\"\"\n",
    "    df['premium_diff'] = df[premium_gap_col] - df[premium_index_col]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•´ì‹œë ˆì´íŠ¸ì™€ ë‚œì´ë„ ê°„ì˜ ë¹„ìœ¨\n",
    "def make_hashrate_to_difficulty_feature(\n",
    "    df: pd.DataFrame, \n",
    "    hashrate_col: str, \n",
    "    difficulty_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    í•´ì‹œë ˆì´íŠ¸ì™€ ë‚œì´ë„ ê°„ì˜ ë¹„ìœ¨ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜\n",
    "    Args:\n",
    "        df (pd.DataFrame): ì…ë ¥ ë°ì´í„°í”„ë ˆì„\n",
    "        hashrate_col (str): í•´ì‹œë ˆì´íŠ¸ ë°ì´í„°ê°€ ì €ì¥ëœ ì—´ ì´ë¦„\n",
    "        difficulty_col (str): ë‚œì´ë„ ë°ì´í„°ê°€ ì €ì¥ëœ ì—´ ì´ë¦„\n",
    "    Returns:\n",
    "        pd.DataFrame: í•´ì‹œë ˆì´íŠ¸ì™€ ë‚œì´ë„ ë¹„ìœ¨ì„ ì €ì¥í•œ ìƒˆë¡œìš´ ì—´ì´ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„\n",
    "    \"\"\"\n",
    "    df['hashrate_to_difficulty'] = df[hashrate_col] / (df[difficulty_col] + 1e-6)  # ë¶„ëª¨ê°€ 0ì´ ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì‘ì€ ê°’ì„ ë”í•¨\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê³µê¸‰ ë³€í™”ìœ¨\n",
    "def make_supply_change_rate_feature(\n",
    "    df: pd.DataFrame, \n",
    "    new_supply_col: str, \n",
    "    total_supply_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ê³µê¸‰ ë³€í™”ìœ¨ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜\n",
    "    Args:\n",
    "        df (pd.DataFrame): ì…ë ¥ ë°ì´í„°í”„ë ˆì„\n",
    "        new_supply_col (str): ìƒˆë¡œìš´ ê³µê¸‰ëŸ‰ ë°ì´í„°ê°€ ì €ì¥ëœ ì—´ ì´ë¦„\n",
    "        total_supply_col (str): ì´ ê³µê¸‰ëŸ‰ ë°ì´í„°ê°€ ì €ì¥ëœ ì—´ ì´ë¦„\n",
    "    Returns:\n",
    "        pd.DataFrame: ê³µê¸‰ ë³€í™”ìœ¨ì„ ì €ì¥í•œ ìƒˆë¡œìš´ ì—´ì´ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„\n",
    "    \"\"\"\n",
    "    df['supply_change_rate'] = df[new_supply_col] / (df[total_supply_col] + 1e-6)  # ë¶„ëª¨ê°€ 0ì´ ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì‘ì€ ê°’ì„ ë”í•¨\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì ìš© \n",
    "df = make_date_features(df, 'ID') # ë‚ ì§œ í”¼ì²˜\n",
    "df = make_diff_change_feature(df, ['open_interest']) # ë³€ë™ì„±, ì°¨ë¶„ í”¼ì²˜ \n",
    "df = make_longshort_ratio_feature(df, 'long_liquidations', 'short_liquidations') # ë¡±/ìˆ ë¹„ìœ¨\n",
    "df = make_liquidation_to_volume_ratio_feature(df, 'long_liquidations', 'short_liquidations', 'taker_buy_volume', 'taker_sell_volume') # ì²­ì‚°/ê±°ë˜ëŸ‰ ë¹„ìœ¨\n",
    "df = make_liquidation_usd_ratio_feature(df, 'long_liquidations_usd', 'short_liquidations_usd') # ì²­ì‚°ëœ USD ë¡±/ìˆ ë¹„ìœ¨\n",
    "df = make_funding_rate_position_change_feature(df, 'funding_rates', 'long_liquidations_usd', 'short_liquidations_usd') # í€ë”© ë¹„ìœ¨ê³¼ ë¡±/ìˆ í¬ì§€ì…˜ ì°¨ì´ ê³±\n",
    "df = make_premium_diff_feature(df, 'coinbase_premium_gap', 'coinbase_premium_index') # í”„ë¦¬ë¯¸ì—„ ê°­ê³¼ í”„ë¦¬ë¯¸ì—„ ì¸ë±ìŠ¤ì˜ ì°¨ì´\n",
    "df = make_hashrate_to_difficulty_feature(df, 'hashrate', 'difficulty') # í•´ì‹œë ˆì´íŠ¸ì™€ ë‚œì´ë„ ê°„ì˜ ë¹„ìœ¨\n",
    "df = make_supply_change_rate_feature(df, 'supply_new', 'supply_total') # ê³µê¸‰ ë³€í™”ìœ¨\n",
    "\n",
    "conti_cols = conti_cols + ['open_interest_pct_change', 'open_interest_diff', 'long_short_ratio', 'liquidation_to_volume_ratio', 'liquidation_usd_ratio', 'funding_rate_position_change', 'premium_diff', 'hashrate_to_difficulty', 'supply_change_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n"
     ]
    }
   ],
   "source": [
    "# ìœ„ì˜ í•¨ìˆ˜ë“¤ì„ ì‹¤í–‰í•œë‹¤.\n",
    "\n",
    "# ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì— ëŒ€í•œ ì§€ìˆ˜ì´ë™í‰ê·  ê³„ì‚°í•´ì„œ ìƒˆë¡œìš´ ì—´ë¡œ í• ë‹¹\n",
    "for c in conti_cols:\n",
    "    df[c+\"_moving_avg_7\"] = EMA(df, c, 7)\n",
    "\n",
    "# ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì— ëŒ€í•œ Wavelet transformì„ ê³„ì‚°í•´ì„œ ìƒˆë¡œìš´ ì—´ë¡œ í• ë‹¹\n",
    "for c in conti_cols:\n",
    "    df[c+\"WT\"] = WT(df, c)\n",
    "\n",
    "# ìµœëŒ€ 24ì‹œê°„ì˜ shift í”¼ì³ë¥¼ ê³„ì‚°\n",
    "shift_list = shift_feature(\n",
    "    df=df, conti_cols=conti_cols, intervals=[_ for _ in range(1, 24)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.concat(shift_list, axis=1)], axis=1)\n",
    "\n",
    "_target = df[\"target\"]\n",
    "df = df.ffill().fillna(-999).assign(target = _target)\n",
    "df = df.loc[:, ~df.columns.duplicated()]\n",
    "df = df.drop(columns=new_features)\n",
    "\n",
    "train_df = df.loc[df[\"_type\"]==\"train\"].drop(columns=[\"_type\"])\n",
    "test_df = df.loc[df[\"_type\"]==\"test\"].drop(columns=[\"_type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lightGBM í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(learning_rate=0.05, n_estimators=30, num_class=4, num_leaves=50,\n",
       "               objective=&#x27;multiclass&#x27;, random_state=42, verbose=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(learning_rate=0.05, n_estimators=30, num_class=4, num_leaves=50,\n",
       "               objective=&#x27;multiclass&#x27;, random_state=42, verbose=-1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(learning_rate=0.05, n_estimators=30, num_class=4, num_leaves=50,\n",
       "               objective='multiclass', random_state=42, verbose=-1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# train_test_split ìœ¼ë¡œ valid set, train set ë¶„ë¦¬\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    train_df.drop([\"target\", \"ID\"], axis = 1), \n",
    "    train_df[\"target\"].astype(int), \n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# LightGBM ëª¨ë¸ ìƒì„± ë° í•™ìŠµ\n",
    "lgb_model = LGBMClassifier(\n",
    "    boosting_type=\"gbdt\",\n",
    "    objective=\"multiclass\",\n",
    "    num_class=4,\n",
    "    num_leaves=50,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=30,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "lgb_model.fit(\n",
    "    x_train, \n",
    "    y_train,\n",
    "    eval_set=[(x_valid, y_valid)],\n",
    "    eval_metric='multi_logloss'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=100, n_jobs=None, num_class=4,\n",
       "              num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=100, n_jobs=None, num_class=4,\n",
       "              num_parallel_tree=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=100, n_jobs=None, num_class=4,\n",
       "              num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# train_test_split ìœ¼ë¡œ valid set, train set ë¶„ë¦¬\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    train_df.drop([\"target\", \"ID\"], axis = 1), \n",
    "    train_df[\"target\"].astype(int), \n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 4,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 100,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# XGBoost ëª¨ë¸ í•™ìŠµ ë° ì„±ëŠ¥ ì²´í¬\n",
    "xgb_model = xgb.XGBClassifier(**xgb_params)\n",
    "xgb_model.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensenble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking\n",
      "  - í‰ê·  ì •í™•ë„: 0.44257990867579905 (+/- 0.018654518918552982)\n",
      "  - AUROC: 0.6156466222603283\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# ë°ì´í„° ì¤€ë¹„\n",
    "X = train_df.drop([\"target\", \"ID\"], axis=1)\n",
    "y = train_df[\"target\"].astype(int)\n",
    "\n",
    "# êµì°¨ ê²€ì¦ì„ ìœ„í•œ KFold ì •ì˜\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 1. Voting ì•™ìƒë¸”\n",
    "voting_hard = VotingClassifier(\n",
    "    estimators=[('lgb', lgb_model), ('xgb', xgb_model), ('rf', rf_model)],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "voting_soft = VotingClassifier(\n",
    "    estimators=[('lgb', lgb_model), ('xgb', xgb_model), ('rf', rf_model)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# 2. Bagging ì•™ìƒë¸”\n",
    "bagging = BaggingClassifier(base_estimator=lgb_model, n_estimators=10, random_state=42)\n",
    "\n",
    "# 3. Stacking ì•™ìƒë¸”\n",
    "\n",
    "class StackingEnsemble(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(y)\n",
    "\n",
    "        self.base_models_ = [list() for _ in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)\n",
    "        \n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        \n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict_proba(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred[:, 1]\n",
    "        \n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self)\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict_proba(X)[:, 1] for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self)\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict_proba(X)[:, 1] for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict_proba(meta_features)\n",
    "\n",
    "stacking = StackingEnsemble(\n",
    "    base_models=(lgb_model, xgb_model, rf_model),\n",
    "    meta_model=LogisticRegression()\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "accuracy_scores = cross_val_score(stacking, X, y, cv=kf, scoring='accuracy', error_score='raise')\n",
    "# AUROC ê³„ì‚°\n",
    "if hasattr(stacking, \"predict_proba\"):\n",
    "    y_pred_proba = cross_val_predict(stacking, X, y, cv=kf, method='predict_proba')\n",
    "    auroc_score = roc_auc_score(y, y_pred_proba, multi_class='ovr')\n",
    "else:\n",
    "    auroc_score = None\n",
    "\n",
    "print(\"Stacking\")\n",
    "print(f\"  - í‰ê·  ì •í™•ë„: {accuracy_scores.mean()} (+/- {accuracy_scores.std() * 2})\")\n",
    "if auroc_score:\n",
    "    print(f\"  - AUROC: {auroc_score}\")\n",
    "else:\n",
    "    print(\"  - AUROC: Not available\")\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM:\n",
      "  - í‰ê·  ì •í™•ë„: 0.44771689497716893 (+/- 0.023166485651079118)\n",
      "  - AUROC: 0.6441506299157067\n",
      "\n",
      "XGBoost:\n",
      "  - í‰ê·  ì •í™•ë„: 0.45468036529680367 (+/- 0.022071852125309502)\n",
      "  - AUROC: 0.6467366784309543\n",
      "\n",
      "RandomForest:\n",
      "  - í‰ê·  ì •í™•ë„: 0.4328767123287672 (+/- 0.021130750615829398)\n",
      "  - AUROC: 0.6331584839403167\n",
      "\n",
      "VotingSoft:\n",
      "  - í‰ê·  ì •í™•ë„: 0.44908675799086756 (+/- 0.019948119779728574)\n",
      "  - AUROC: 0.6547739170883\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/miniconda/envs/ensenble/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/data/ephemeral/home/miniconda/envs/ensenble/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/data/ephemeral/home/miniconda/envs/ensenble/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/data/ephemeral/home/miniconda/envs/ensenble/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/data/ephemeral/home/miniconda/envs/ensenble/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/data/ephemeral/home/miniconda/envs/ensenble/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/data/ephemeral/home/miniconda/envs/ensenble/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/data/ephemeral/home/miniconda/envs/ensenble/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/data/ephemeral/home/miniconda/envs/ensenble/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/data/ephemeral/home/miniconda/envs/ensenble/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging:\n",
      "  - í‰ê·  ì •í™•ë„: 0.45547945205479456 (+/- 0.029495384218664225)\n",
      "  - AUROC: 0.6549400279939801\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (VotingHard ì œì™¸)\n",
    "models = [lgb_model, xgb_model, rf_model, voting_soft, bagging]\n",
    "model_names = ['LightGBM', 'XGBoost', 'RandomForest', 'VotingSoft', 'Bagging']\n",
    "\n",
    "# ê° ëª¨ë¸ì˜ êµì°¨ ê²€ì¦ ìˆ˜í–‰\n",
    "for name, model in zip(model_names, models):\n",
    "    # ì •í™•ë„ ê³„ì‚°\n",
    "    accuracy_scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n",
    "    \n",
    "    # AUROC ê³„ì‚°\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_pred_proba = cross_val_predict(model, X, y, cv=kf, method='predict_proba')\n",
    "        auroc_score = roc_auc_score(y, y_pred_proba, multi_class='ovr')\n",
    "    else:\n",
    "        auroc_score = None\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  - í‰ê·  ì •í™•ë„: {accuracy_scores.mean()} (+/- {accuracy_scores.std() * 2})\")\n",
    "    if auroc_score:\n",
    "        print(f\"  - AUROC: {auroc_score}\")\n",
    "    else:\n",
    "        print(\"  - AUROC: Not available\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ìµœì¢… ëª¨ë¸ ì„ íƒ ë° ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµ (AUROC ê¸°ì¤€)\n",
    "best_model = models[np.argmax([\n",
    "    roc_auc_score(y, cross_val_predict(model, X, y, cv=kf, method='predict_proba'), multi_class='ovr')\n",
    "    if hasattr(model, \"predict_proba\") else 0 \n",
    "    for model in models\n",
    "])]\n",
    "\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡\n",
    "X_test = test_df.drop([\"target\", \"ID\"], axis=1)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "if hasattr(best_model, \"predict_proba\"):\n",
    "    y_test_pred_proba = best_model.predict_proba(X_test)\n",
    "\n",
    "# ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "submission = submission.assign(target=y_test_pred)\n",
    "submission.to_csv(\"output_best_ensemble.csv\", index=False)\n",
    "\n",
    "print(f\"Best model: {type(best_model).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì•™ìƒë¸”ì˜ inference & save ê³¼ì •."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('datawrg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "ab2bf47c83e23e79da7154310486cd6f2111092cec5daef28d72dd2b3b6f44d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
