{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 중에서 안깔린 라이브러리가 있을 수 있음\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import lightgbm as lgb\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../data/\"\n",
    "\n",
    "train = pd.read_csv(path+\"train.csv\").assign(_type=\"train\")\n",
    "test = pd.read_csv(path+\"test.csv\").assign(_type=\"test\")\n",
    "submission = pd.read_csv(path+\"test.csv\")\n",
    "df = pd.concat([train, test], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 원래 베이스라인 코드 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # HOURLY_ 로 시작하는 .csv 파일 이름을 file_names 에 할딩\n",
    "# file_names: List[str] = [\n",
    "#     f for f in os.listdir(data_path) if f.startswith(\"HOURLY_\") and f.endswith(\".csv\")\n",
    "# ]\n",
    "\n",
    "# # 파일명 : 데이터프레임으로 딕셔너리 형태로 저장\n",
    "# file_dict: Dict[str, pd.DataFrame] = {\n",
    "#     f.replace(\".csv\", \"\"): pd.read_csv(os.path.join(data_path, f)) for f in file_names\n",
    "# }\n",
    "\n",
    "# for _file_name, _df in tqdm(file_dict.items()):\n",
    "#     # 열 이름 중복 방지를 위해 {_file_name.lower()}_{col.lower()}로 변경, datetime 열을 ID로 변경\n",
    "#     _rename_rule = {\n",
    "#         col: f\"{_file_name.lower()}_{col.lower()}\" if col != \"datetime\" else \"ID\"\n",
    "#         for col in _df.columns\n",
    "#     }\n",
    "#     _df = _df.rename(_rename_rule, axis=1)\n",
    "#     df = df.merge(_df, on=\"ID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 밑의 한 셀이 바꾼 부분 (이 바꾼 부분을 전제로 뒤의 코드가 실행됨.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HOURLY_NETWORK-DATA_SUPPLY.csv',\n",
       " 'HOURLY_MARKET-DATA_COINBASE-PREMIUM-INDEX.csv',\n",
       " 'HOURLY_NETWORK-DATA_DIFFICULTY.csv',\n",
       " 'HOURLY_NETWORK-DATA_ADDRESSES-COUNT.csv',\n",
       " 'HOURLY_NETWORK-DATA_UTXO-COUNT.csv',\n",
       " 'HOURLY_NETWORK-DATA_VELOCITY.csv',\n",
       " 'HOURLY_MARKET-DATA_LIQUIDATIONS_ALL_EXCHANGE_ALL_SYMBOL.csv',\n",
       " 'HOURLY_NETWORK-DATA_FEES.csv',\n",
       " 'HOURLY_NETWORK-DATA_HASHRATE.csv',\n",
       " 'HOURLY_MARKET-DATA_FUNDING-RATES_ALL_EXCHANGE.csv',\n",
       " 'HOURLY_NETWORK-DATA_BLOCKREWARD.csv',\n",
       " 'HOURLY_NETWORK-DATA_FEES-TRANSACTION.csv',\n",
       " 'HOURLY_NETWORK-DATA_TRANSACTIONS-COUNT.csv',\n",
       " 'HOURLY_MARKET-DATA_OPEN-INTEREST_ALL_EXCHANGE_ALL_SYMBOL.csv',\n",
       " 'HOURLY_NETWORK-DATA_BLOCK-BYTES.csv',\n",
       " 'HOURLY_NETWORK-DATA_TOKENS-TRANSFERRED.csv',\n",
       " 'HOURLY_MARKET-DATA_PRICE-OHLCV_ALL_EXCHANGE_SPOT_BTC_USD.csv',\n",
       " 'HOURLY_NETWORK-DATA_BLOCK-INTERVAL.csv',\n",
       " 'HOURLY_NETWORK-DATA_BLOCK-COUNT.csv',\n",
       " 'HOURLY_MARKET-DATA_TAKER-BUY-SELL-STATS_ALL_EXCHANGE.csv']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 30.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# HOURLY_로 시작하는 모든 csv 파일 불러오기\n",
    "prefixes = [\"HOURLY_NETWORK-DATA_\",\n",
    "            \"HOURLY_MARKET-DATA_PRICE-OHLCV_ALL_EXCHANGE_SPOT_BTC_USD\",\n",
    "            \"HOURLY_MARKET-DATA_COINBASE-PREMIUM-INDEX\",\n",
    "            \"HOURLY_MARKET-DATA_FUNDING-RATES_ALL_EXCHANGE\",\n",
    "            \"HOURLY_MARKET-DATA_LIQUIDATIONS_ALL_EXCHANGE_ALL_SYMBOL\",\n",
    "            \"HOURLY_MARKET-DATA_OPEN-INTEREST_ALL_EXCHANGE_ALL_SYMBOL\",\n",
    "            \"HOURLY_MARKET-DATA_TAKER-BUY-SELL-STATS_ALL_EXCHANGE\"]\n",
    "\n",
    "file_names = [\n",
    "    f for f in os.listdir(path)\n",
    "    if any(f.startswith(prefix) for prefix in prefixes) and f.endswith(\".csv\")\n",
    "]\n",
    "display(file_names)\n",
    "file_dict: Dict[str, pd.DataFrame] = {\n",
    "    f.replace(\".csv\", \"\"): pd.read_csv(path+f) for f in file_names\n",
    "}\n",
    "\n",
    "\n",
    "for _file_name, _df in tqdm(file_dict.items()):\n",
    "    _rename_rule = {\n",
    "        col: f\"{col.lower()}\" if col != \"datetime\" else \"ID\"\n",
    "        for col in _df.columns\n",
    "    }\n",
    "    _df = _df.rename(_rename_rule, axis=1)\n",
    "    df = df.merge(_df, on=\"ID\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 결측치 (아거도 추가된 부분)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "volume                         2792\n",
       "close                          2792\n",
       "block_bytes                      54\n",
       "funding_rates                    32\n",
       "taker_buy_sell_ratio             31\n",
       "taker_sell_ratio                 31\n",
       "taker_buy_ratio                  31\n",
       "taker_sell_volume                31\n",
       "taker_buy_volume                 31\n",
       "transactions_count_mean          28\n",
       "supply_new                       28\n",
       "supply_total                     28\n",
       "fees_transaction_mean_usd        24\n",
       "fees_block_mean                  24\n",
       "fees_block_mean_usd              24\n",
       "fees_reward_percent              24\n",
       "difficulty                       24\n",
       "tokens_transferred_mean          24\n",
       "block_interval                   24\n",
       "fees_transaction_mean            24\n",
       "coinbase_premium_gap              6\n",
       "coinbase_premium_index            6\n",
       "transactions_count_total          4\n",
       "open_interest                     4\n",
       "block_count                       4\n",
       "utxo_count                        1\n",
       "addresses_count_sender            0\n",
       "addresses_count_receiver          0\n",
       "tokens_transferred_median         0\n",
       "tokens_transferred_total          0\n",
       "addresses_count_active            0\n",
       "long_liquidations_usd             0\n",
       "velocity_supply_total             0\n",
       "short_liquidations_usd            0\n",
       "fees_transaction_median_usd       0\n",
       "fees_transaction_median           0\n",
       "blockreward_usd                   0\n",
       "long_liquidations                 0\n",
       "short_liquidations                0\n",
       "hashrate                          0\n",
       "fees_total_usd                    0\n",
       "fees_total                        0\n",
       "blockreward                       0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "volume                         0.241690\n",
       "close                          0.241690\n",
       "block_bytes                    0.004675\n",
       "funding_rates                  0.002770\n",
       "taker_buy_sell_ratio           0.002684\n",
       "taker_sell_ratio               0.002684\n",
       "taker_buy_ratio                0.002684\n",
       "taker_sell_volume              0.002684\n",
       "taker_buy_volume               0.002684\n",
       "transactions_count_mean        0.002424\n",
       "supply_new                     0.002424\n",
       "supply_total                   0.002424\n",
       "fees_transaction_mean_usd      0.002078\n",
       "fees_block_mean                0.002078\n",
       "fees_block_mean_usd            0.002078\n",
       "fees_reward_percent            0.002078\n",
       "difficulty                     0.002078\n",
       "tokens_transferred_mean        0.002078\n",
       "block_interval                 0.002078\n",
       "fees_transaction_mean          0.002078\n",
       "coinbase_premium_gap           0.000519\n",
       "coinbase_premium_index         0.000519\n",
       "transactions_count_total       0.000346\n",
       "open_interest                  0.000346\n",
       "block_count                    0.000346\n",
       "utxo_count                     0.000087\n",
       "addresses_count_sender         0.000000\n",
       "addresses_count_receiver       0.000000\n",
       "tokens_transferred_median      0.000000\n",
       "tokens_transferred_total       0.000000\n",
       "addresses_count_active         0.000000\n",
       "long_liquidations_usd          0.000000\n",
       "velocity_supply_total          0.000000\n",
       "short_liquidations_usd         0.000000\n",
       "fees_transaction_median_usd    0.000000\n",
       "fees_transaction_median        0.000000\n",
       "blockreward_usd                0.000000\n",
       "long_liquidations              0.000000\n",
       "short_liquidations             0.000000\n",
       "hashrate                       0.000000\n",
       "fees_total_usd                 0.000000\n",
       "fees_total                     0.000000\n",
       "blockreward                    0.000000\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_new = df.drop([\"ID\", \"target\", \"_type\"], axis = 1)\n",
    "\n",
    "# 결측치 확인\n",
    "missing_data = df_new.isnull().sum()\n",
    "\n",
    "# 결측치 비율 확인\n",
    "missing_ratio = df_new.isnull().mean()\n",
    "\n",
    "\n",
    "display(missing_data.sort_values(ascending=False))\n",
    "display(missing_ratio.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supply_total                   0\n",
      "supply_new                     0\n",
      "coinbase_premium_gap           0\n",
      "coinbase_premium_index         0\n",
      "difficulty                     0\n",
      "addresses_count_active         0\n",
      "addresses_count_sender         0\n",
      "addresses_count_receiver       0\n",
      "utxo_count                     0\n",
      "velocity_supply_total          0\n",
      "long_liquidations              0\n",
      "short_liquidations             0\n",
      "long_liquidations_usd          0\n",
      "short_liquidations_usd         0\n",
      "fees_block_mean                0\n",
      "fees_block_mean_usd            0\n",
      "fees_total                     0\n",
      "fees_total_usd                 0\n",
      "fees_reward_percent            0\n",
      "hashrate                       0\n",
      "funding_rates                  0\n",
      "blockreward                    0\n",
      "blockreward_usd                0\n",
      "fees_transaction_mean          0\n",
      "fees_transaction_mean_usd      0\n",
      "fees_transaction_median        0\n",
      "fees_transaction_median_usd    0\n",
      "transactions_count_total       0\n",
      "transactions_count_mean        0\n",
      "open_interest                  0\n",
      "block_bytes                    0\n",
      "tokens_transferred_total       0\n",
      "tokens_transferred_mean        0\n",
      "tokens_transferred_median      0\n",
      "close                          0\n",
      "volume                         0\n",
      "block_interval                 0\n",
      "block_count                    0\n",
      "taker_buy_volume               0\n",
      "taker_sell_volume              0\n",
      "taker_buy_ratio                0\n",
      "taker_sell_ratio               0\n",
      "taker_buy_sell_ratio           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. 결측치 처리 - 앞/뒤 보간법 (forward/backward fill)\n",
    "\n",
    "# df_ffill = df_new.fillna(method='ffill')  # 앞의 값을 사용하여 결측치 채움\n",
    "# df_bfill = df_new.fillna(method='bfill')  # 뒤의 값을 사용하여 결측치 채움\n",
    "\n",
    "# 2. 결측치 처리 - 선형 보간법 (linear interpolation)\n",
    "df_interpolated = df_new.interpolate(method='linear')\n",
    "\n",
    "# 3. 결측치 처리 - 평균값으로 대체\n",
    "# df_mean_filled = df_new.fillna(eda_df.mean())\n",
    "\n",
    "df.update(df_interpolated)\n",
    "\n",
    "# 결측치 처리 후 확인\n",
    "print(df_interpolated.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 데이터를 로드할 때, (제공 받은)베이스라인 코드에서와 같이 거래소와 암호화폐 종류를 불러오도록 설정을 했다면\n",
    "\n",
    "아래 코드는 작동하지 않음\n",
    "\n",
    "거래소와 암호화폐 종류를 고려하지 않은 코드임을 유의.\n",
    "\n",
    "(수정을 해야할 것 같음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 독립변수에서 내가 사용할 독립변수 고르기\n",
    "\n",
    "\n",
    "# 전체 feature\n",
    "features = df.columns\n",
    "# 내가 쓸 feature\n",
    "fe = ['ID', 'target', '_type', 'funding_rates', 'open_interest',\n",
    "       'taker_buy_volume', 'taker_sell_volume', 'taker_buy_ratio',\n",
    "       'taker_sell_ratio', 'taker_buy_sell_ratio', 'difficulty',\n",
    "       'transactions_count_total', 'transactions_count_mean', 'block_count',\n",
    "       'fees_transaction_mean', 'fees_transaction_mean_usd',\n",
    "       'fees_transaction_median', 'fees_transaction_median_usd',\n",
    "       'fees_block_mean', 'fees_block_mean_usd', 'fees_total',\n",
    "       'fees_total_usd', 'fees_reward_percent', 'hashrate', 'utxo_count',\n",
    "       'tokens_transferred_total', 'tokens_transferred_mean',\n",
    "       'tokens_transferred_median', 'block_interval', 'velocity_supply_total',\n",
    "       'supply_total', 'supply_new', 'addresses_count_active',\n",
    "       'addresses_count_sender', 'addresses_count_receiver',\n",
    "       'blockreward', 'blockreward_usd', 'buy_sell_volume_ratio']\n",
    "\n",
    "# new_features에는 내가 사용하지 않을 feature들이 담기고 나중에 drop됨. 때문에, 이름이 좀 잘못된 듯\n",
    "new_features = [x for x in features if x not in fe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda 에서 파악한 차이와 차이의 음수, 양수 여부를 새로운 피쳐로 생성\n",
    "df = df.assign(\n",
    "    liquidation_diff=df[\"long_liquidations\"] - df[\"short_liquidations\"],\n",
    "    liquidation_usd_diff=df[\"long_liquidations_usd\"] - df[\"short_liquidations_usd\"],\n",
    "    volume_diff=df[\"taker_buy_volume\"] - df[\"taker_sell_volume\"],\n",
    "    liquidation_diffg=np.sign(df[\"long_liquidations\"] - df[\"short_liquidations\"]),\n",
    "    liquidation_usd_diffg=np.sign(df[\"long_liquidations_usd\"] - df[\"short_liquidations_usd\"]),\n",
    "    volume_diffg=np.sign(df[\"taker_buy_volume\"] - df[\"taker_sell_volume\"]),\n",
    "    buy_sell_volume_ratio=df[\"taker_buy_volume\"] / (df[\"taker_sell_volume\"] + 1),\n",
    ")\n",
    "\n",
    "# category, continuous 열을 따로 할당해둠\n",
    "category_cols = [\"liquidation_diffg\", \"liquidation_usd_diffg\", \"volume_diffg\"]\n",
    "conti_cols = [_ for _ in (df.columns)[3:] if _ not in [\"ID\", \"target\", \"_type\"]] + [\n",
    "    \"buy_sell_volume_ratio\",\n",
    "    \"liquidation_diff\",\n",
    "    \"liquidation_usd_diff\",\n",
    "    \"volume_diff\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_feature(\n",
    "    df: pd.DataFrame,\n",
    "    conti_cols: List[str],\n",
    "    intervals: List[int],\n",
    ") -> List[pd.Series]:\n",
    "    \"\"\"\n",
    "    연속형 변수의 shift feature 생성\n",
    "    Args:\n",
    "        df (pd.DataFrame)\n",
    "        conti_cols (List[str]): continuous colnames\n",
    "        intervals (List[int]): shifted intervals\n",
    "    Return:\n",
    "        List[pd.Series]\n",
    "    \"\"\"\n",
    "    df_shift_dict = [\n",
    "        df[conti_col].shift(interval).rename(f\"{conti_col}_{interval}\")\n",
    "        for conti_col in conti_cols\n",
    "        for interval in intervals\n",
    "    ]\n",
    "    return df_shift_dict\n",
    "\n",
    "# 지수 이동 평균\n",
    "def EMA(df, col, span=2):\n",
    "    return df[col].ewm(span=span).mean()\n",
    "\n",
    "# Wavlet Transform\n",
    "def WT(df, col, wavelet='db5', th=0.6):\n",
    "    signal = df[col].values\n",
    "    th = th*np.nanmax(signal)\n",
    "    coef = pywt.wavedec(signal, wavelet, mode=\"per\" )\n",
    "    coef[1:] = (pywt.threshold(i, value=th, mode=\"soft\" ) for i in coef[1:])\n",
    "    reconstructed = pywt.waverec(coef, wavelet, mode=\"per\" )\n",
    "    return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 🔥 피처엔지니어링 함수 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜 관련 생성\n",
    "def make_date_features(df: pd.DataFrame, date_column: str) -> Tuple[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    입력된 데이터프레임의 특정 날짜 열을 기준으로 연도, 월, 주, 요일, 시간을 추출하여 새로운 피처로 추가.\n",
    "    Args:\n",
    "        df (pd.DataFrame): 날짜 컬럼을 포함하는 데이터프레임.\n",
    "        date_column (str): 날짜 정보가 담긴 열 이름.\n",
    "    Returns:\n",
    "        pd.DataFrame: 날짜 관련 피처가 추가된 데이터프레임 반환.\n",
    "    \"\"\"\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "    df['year'] = df[date_column].dt.year  # 연도\n",
    "    df['month'] = df[date_column].dt.month  # 월 \n",
    "    df['week'] = df[date_column].dt.isocalendar().week  # 주\n",
    "    df['day_of_week'] = df[date_column].dt.dayofweek  # 요일 \n",
    "    df['hour'] = df[date_column].dt.hour  # 시간 \n",
    "    date_columns = [date_column, 'year', 'month', 'week', 'day_of_week', 'hour']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변동성, 차분 피처 생성 함수 \n",
    "def make_diff_change_feature(\n",
    "    df: pd.DataFrame, \n",
    "    columns_list: List[str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    주어진 변수들에 대한 변동성, 차분 피처를 생성하고, 결측값을 처리하는 함수.\n",
    "    Args:\n",
    "        df (pd.DataFrame): 데이터를 포함한 데이터프레임.\n",
    "        columns_list (List[str]): 변동성과 차분 피처를 추가하고자 하는 열 이름 목록.\n",
    "    Returns:\n",
    "        pd.DataFrame: 변동성과 차분 피처가 추가된 데이터프레임.\n",
    "    \"\"\"  \n",
    "    new_features = {}  # 새로 추가할 열을 저장할 딕셔너리\n",
    "    for col in columns_list:\n",
    "        if col in df.columns:\n",
    "            pct_change_col = f'{col}_pct_change'\n",
    "            diff_col = f'{col}_diff'\n",
    "            new_features[pct_change_col] = df[col].pct_change(fill_method=None)\n",
    "            new_features[diff_col] = df[col].diff()\n",
    "        else:\n",
    "            print(f\"Error: Cannot find '{col}' column\")\n",
    "    new_features_df = pd.DataFrame(new_features, index=df.index) # 새 피처를 데이터프레임으로\n",
    "    new_features_df = new_features_df.ffill().bfill()  # 결측값 처리 (고정: ffill, bfill)\n",
    "    df = pd.concat([df, new_features_df], axis=1)  # 기존 데이터프레임에 추가\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 롱/숏 비율\n",
    "def make_longshort_ratio_feature(\n",
    "    df: pd.DataFrame, \n",
    "    long_col: str, \n",
    "    short_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    롱/숏 비율을 계산하는 함수\n",
    "    Args:\n",
    "        df (pd.DataFrame): 입력 데이터프레임\n",
    "        long_col (str): 롱(liquidations) 데이터가 저장된 열 이름\n",
    "        short_col (str): 숏(liquidations) 데이터가 저장된 열 이름\n",
    "    Returns:\n",
    "        pd.DataFrame: 롱/숏 비율이 저장된 새로운 열이 추가된 데이터프레임\n",
    "    \"\"\"\n",
    "    df['long_short_ratio'] = df[long_col] / (df[short_col] + 1e-6)  # 분모가 0이 되는 것을 방지하기 위해 작은 값을 더함\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 청산/거래량 비율\n",
    "def make_liquidation_to_volume_ratio_feature(\n",
    "    df: pd.DataFrame, \n",
    "    long_col: str, \n",
    "    short_col: str, \n",
    "    buy_volume_col: str, \n",
    "    sell_volume_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    청산/거래량 비율을 계산하는 함수\n",
    "    Args:\n",
    "        df (pd.DataFrame): 입력 데이터프레임\n",
    "        long_col (str): 롱(liquidations) 데이터가 저장된 열 이름\n",
    "        short_col (str): 숏(liquidations) 데이터가 저장된 열 이름\n",
    "        buy_volume_col (str): 매수 거래량이 저장된 열 이름\n",
    "        sell_volume_col (str): 매도 거래량이 저장된 열 이름\n",
    "    Returns:\n",
    "        pd.DataFrame: 청산/거래량 비율을 저장한 새로운 열이 추가된 데이터프레임\n",
    "    \"\"\"\n",
    "    df['liquidation_to_volume_ratio'] = (\n",
    "        (df[long_col] + df[short_col]) / \n",
    "        (df[buy_volume_col] + df[sell_volume_col] + 1e-6)  # 분모가 0이 되는 것을 방지하기 위해 작은 값을 더함\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 청산된 USD 롱/숏 비율\n",
    "def make_liquidation_usd_ratio_feature(\n",
    "    df: pd.DataFrame, \n",
    "    long_usd_col: str, \n",
    "    short_usd_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    청산된 USD 롱/숏 비율을 계산하는 함수\n",
    "    Args:\n",
    "        df (pd.DataFrame): 입력 데이터프레임\n",
    "        long_usd_col (str): 롱(liquidations) USD 데이터가 저장된 열 이름\n",
    "        short_usd_col (str): 숏(liquidations) USD 데이터가 저장된 열 이름 \n",
    "    Returns:\n",
    "        pd.DataFrame: 청산된 USD 롱/숏 비율을 저장한 새로운 열이 추가된 데이터프레임\n",
    "    \"\"\"\n",
    "    df['liquidation_usd_ratio'] = df[long_usd_col] / (df[short_usd_col] + 1e-6)  # 분모가 0이 되는 것을 방지하기 위해 작은 값을 더함\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 펀딩 비율과 롱/숏 포지션 차이 곱\n",
    "def make_funding_rate_position_change_feature(\n",
    "    df: pd.DataFrame, \n",
    "    funding_rate_col: str, \n",
    "    long_liquidations_col: str, \n",
    "    short_liquidations_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    펀딩 비율과 롱/숏 포지션 차이를 곱하여 포지션 변화를 계산하는 함수\n",
    "    Args:\n",
    "        df (pd.DataFrame): 입력 데이터프레임\n",
    "        funding_rate_col (str): 펀딩 비율 데이터가 저장된 열 이름\n",
    "        long_liquidations_col (str): 롱 청산 데이터가 저장된 열 이름\n",
    "        short_liquidations_col (str): 숏 청산 데이터가 저장된 열 이름\n",
    "    Returns:\n",
    "        pd.DataFrame: 펀딩 비율에 따른 포지션 변화를 저장한 새로운 열이 추가된 데이터프레임\n",
    "    \"\"\"\n",
    "    df['funding_rate_position_change'] = df[funding_rate_col] * (\n",
    "        df[long_liquidations_col] - df[short_liquidations_col]\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프리미엄 갭과 프리미엄 인덱스의 차이\n",
    "def make_premium_diff_feature(\n",
    "    df: pd.DataFrame, \n",
    "    premium_gap_col: str, \n",
    "    premium_index_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    프리미엄 갭과 프리미엄 인덱스의 차이를 계산하는 함수\n",
    "    Args:\n",
    "        df (pd.DataFrame): 입력 데이터프레임\n",
    "        premium_gap_col (str): 프리미엄 갭 데이터가 저장된 열 이름\n",
    "        premium_index_col (str): 프리미엄 인덱스 데이터가 저장된 열 이름\n",
    "    Returns:\n",
    "        pd.DataFrame: 프리미엄 갭과 프리미엄 인덱스의 차이를 저장한 새로운 열이 추가된 데이터프레임\n",
    "    \"\"\"\n",
    "    df['premium_diff'] = df[premium_gap_col] - df[premium_index_col]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해시레이트와 난이도 간의 비율\n",
    "def make_hashrate_to_difficulty_feature(\n",
    "    df: pd.DataFrame, \n",
    "    hashrate_col: str, \n",
    "    difficulty_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    해시레이트와 난이도 간의 비율을 계산하는 함수\n",
    "    Args:\n",
    "        df (pd.DataFrame): 입력 데이터프레임\n",
    "        hashrate_col (str): 해시레이트 데이터가 저장된 열 이름\n",
    "        difficulty_col (str): 난이도 데이터가 저장된 열 이름\n",
    "    Returns:\n",
    "        pd.DataFrame: 해시레이트와 난이도 비율을 저장한 새로운 열이 추가된 데이터프레임\n",
    "    \"\"\"\n",
    "    df['hashrate_to_difficulty'] = df[hashrate_col] / (df[difficulty_col] + 1e-6)  # 분모가 0이 되는 것을 방지하기 위해 작은 값을 더함\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공급 변화율\n",
    "def make_supply_change_rate_feature(\n",
    "    df: pd.DataFrame, \n",
    "    new_supply_col: str, \n",
    "    total_supply_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    공급 변화율을 계산하는 함수\n",
    "    Args:\n",
    "        df (pd.DataFrame): 입력 데이터프레임\n",
    "        new_supply_col (str): 새로운 공급량 데이터가 저장된 열 이름\n",
    "        total_supply_col (str): 총 공급량 데이터가 저장된 열 이름\n",
    "    Returns:\n",
    "        pd.DataFrame: 공급 변화율을 저장한 새로운 열이 추가된 데이터프레임\n",
    "    \"\"\"\n",
    "    df['supply_change_rate'] = df[new_supply_col] / (df[total_supply_col] + 1e-6)  # 분모가 0이 되는 것을 방지하기 위해 작은 값을 더함\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 적용 \n",
    "df = make_date_features(df, 'ID') # 날짜 피처\n",
    "df = make_diff_change_feature(df, ['open_interest']) # 변동성, 차분 피처 \n",
    "df = make_longshort_ratio_feature(df, 'long_liquidations', 'short_liquidations') # 롱/숏 비율\n",
    "df = make_liquidation_to_volume_ratio_feature(df, 'long_liquidations', 'short_liquidations', 'taker_buy_volume', 'taker_sell_volume') # 청산/거래량 비율\n",
    "df = make_liquidation_usd_ratio_feature(df, 'long_liquidations_usd', 'short_liquidations_usd') # 청산된 USD 롱/숏 비율\n",
    "df = make_funding_rate_position_change_feature(df, 'funding_rates', 'long_liquidations_usd', 'short_liquidations_usd') # 펀딩 비율과 롱/숏 포지션 차이 곱\n",
    "df = make_premium_diff_feature(df, 'coinbase_premium_gap', 'coinbase_premium_index') # 프리미엄 갭과 프리미엄 인덱스의 차이\n",
    "df = make_hashrate_to_difficulty_feature(df, 'hashrate', 'difficulty') # 해시레이트와 난이도 간의 비율\n",
    "df = make_supply_change_rate_feature(df, 'supply_new', 'supply_total') # 공급 변화율\n",
    "\n",
    "conti_cols = conti_cols + ['open_interest_pct_change', 'open_interest_diff', 'long_short_ratio', 'liquidation_to_volume_ratio', 'liquidation_usd_ratio', 'funding_rate_position_change', 'premium_diff', 'hashrate_to_difficulty', 'supply_change_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_782110/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n"
     ]
    }
   ],
   "source": [
    "# 위의 함수들을 실행한다.\n",
    "\n",
    "# 모든 수치형 컬럼에 대한 지수이동평균 계산해서 새로운 열로 할당\n",
    "for c in conti_cols:\n",
    "    df[c+\"_moving_avg_7\"] = EMA(df, c, 7)\n",
    "\n",
    "# 모든 수치형 컬럼에 대한 Wavelet transform을 계산해서 새로운 열로 할당\n",
    "for c in conti_cols:\n",
    "    df[c+\"WT\"] = WT(df, c)\n",
    "\n",
    "# 최대 24시간의 shift 피쳐를 계산\n",
    "shift_list = shift_feature(\n",
    "    df=df, conti_cols=conti_cols, intervals=[_ for _ in range(1, 24)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.concat(shift_list, axis=1)], axis=1)\n",
    "\n",
    "_target = df[\"target\"]\n",
    "df = df.ffill().fillna(-999).assign(target = _target)\n",
    "df = df.loc[:, ~df.columns.duplicated()]\n",
    "df = df.drop(columns=new_features)\n",
    "\n",
    "train_df = df.loc[df[\"_type\"]==\"train\"].drop(columns=[\"_type\"])\n",
    "test_df = df.loc[df[\"_type\"]==\"test\"].drop(columns=[\"_type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lightGBM 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(learning_rate=0.05, n_estimators=30, num_class=4, num_leaves=50,\n",
       "               objective=&#x27;multiclass&#x27;, random_state=42, verbose=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(learning_rate=0.05, n_estimators=30, num_class=4, num_leaves=50,\n",
       "               objective=&#x27;multiclass&#x27;, random_state=42, verbose=-1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(learning_rate=0.05, n_estimators=30, num_class=4, num_leaves=50,\n",
       "               objective='multiclass', random_state=42, verbose=-1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# train_test_split 으로 valid set, train set 분리\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    train_df.drop([\"target\", \"ID\"], axis = 1), \n",
    "    train_df[\"target\"].astype(int), \n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# LightGBM 모델 생성 및 학습\n",
    "lgb_model = LGBMClassifier(\n",
    "    boosting_type=\"gbdt\",\n",
    "    objective=\"multiclass\",\n",
    "    num_class=4,\n",
    "    num_leaves=50,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=30,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "lgb_model.fit(\n",
    "    x_train, \n",
    "    y_train,\n",
    "    eval_set=[(x_valid, y_valid)],\n",
    "    eval_metric='multi_logloss'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=100, n_jobs=None, num_class=4,\n",
       "              num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=100, n_jobs=None, num_class=4,\n",
       "              num_parallel_tree=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=100, n_jobs=None, num_class=4,\n",
       "              num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# train_test_split 으로 valid set, train set 분리\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    train_df.drop([\"target\", \"ID\"], axis = 1), \n",
    "    train_df[\"target\"].astype(int), \n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 4,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 100,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# XGBoost 모델 학습 및 성능 체크\n",
    "xgb_model = xgb.XGBClassifier(**xgb_params)\n",
    "xgb_model.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensenble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking\n",
      "  - 평균 정확도: 0.44257990867579905 (+/- 0.018654518918552982)\n",
      "  - AUROC: 0.6156466222603283\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# 데이터 준비\n",
    "X = train_df.drop([\"target\", \"ID\"], axis=1)\n",
    "y = train_df[\"target\"].astype(int)\n",
    "\n",
    "# 교차 검증을 위한 KFold 정의\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 1. Voting 앙상블\n",
    "voting_hard = VotingClassifier(\n",
    "    estimators=[('lgb', lgb_model), ('xgb', xgb_model), ('rf', rf_model)],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "voting_soft = VotingClassifier(\n",
    "    estimators=[('lgb', lgb_model), ('xgb', xgb_model), ('rf', rf_model)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# 2. Bagging 앙상블\n",
    "bagging = BaggingClassifier(base_estimator=lgb_model, n_estimators=10, random_state=42)\n",
    "\n",
    "# 3. Stacking 앙상블\n",
    "\n",
    "class StackingEnsemble(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(y)\n",
    "\n",
    "        self.base_models_ = [list() for _ in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)\n",
    "        \n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        \n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict_proba(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred[:, 1]\n",
    "        \n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self)\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict_proba(X)[:, 1] for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self)\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict_proba(X)[:, 1] for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict_proba(meta_features)\n",
    "\n",
    "stacking = StackingEnsemble(\n",
    "    base_models=(lgb_model, xgb_model, rf_model),\n",
    "    meta_model=LogisticRegression()\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "accuracy_scores = cross_val_score(stacking, X, y, cv=kf, scoring='accuracy', error_score='raise')\n",
    "# AUROC 계산\n",
    "if hasattr(stacking, \"predict_proba\"):\n",
    "    y_pred_proba = cross_val_predict(stacking, X, y, cv=kf, method='predict_proba')\n",
    "    auroc_score = roc_auc_score(y, y_pred_proba, multi_class='ovr')\n",
    "else:\n",
    "    auroc_score = None\n",
    "\n",
    "print(\"Stacking\")\n",
    "print(f\"  - 평균 정확도: {accuracy_scores.mean()} (+/- {accuracy_scores.std() * 2})\")\n",
    "if auroc_score:\n",
    "    print(f\"  - AUROC: {auroc_score}\")\n",
    "else:\n",
    "    print(\"  - AUROC: Not available\")\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM:\n",
      "  - 평균 정확도: 0.44771689497716893 (+/- 0.023166485651079118)\n",
      "  - AUROC: 0.6441506299157067\n",
      "\n",
      "XGBoost:\n",
      "  - 평균 정확도: 0.45468036529680367 (+/- 0.022071852125309502)\n",
      "  - AUROC: 0.6467366784309543\n",
      "\n",
      "RandomForest:\n",
      "  - 평균 정확도: 0.4328767123287672 (+/- 0.021130750615829398)\n",
      "  - AUROC: 0.6331584839403167\n",
      "\n",
      "VotingSoft:\n",
      "  - 평균 정확도: 0.44908675799086756 (+/- 0.019948119779728574)\n",
      "  - AUROC: 0.6547739170883\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/miniconda/envs/ensenble/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/data/ephemeral/home/miniconda/envs/ensenble/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/data/ephemeral/home/miniconda/envs/ensenble/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/data/ephemeral/home/miniconda/envs/ensenble/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/data/ephemeral/home/miniconda/envs/ensenble/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/data/ephemeral/home/miniconda/envs/ensenble/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/data/ephemeral/home/miniconda/envs/ensenble/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/data/ephemeral/home/miniconda/envs/ensenble/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/data/ephemeral/home/miniconda/envs/ensenble/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/data/ephemeral/home/miniconda/envs/ensenble/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging:\n",
      "  - 평균 정확도: 0.45547945205479456 (+/- 0.029495384218664225)\n",
      "  - AUROC: 0.6549400279939801\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# 모델 리스트 (VotingHard 제외)\n",
    "models = [lgb_model, xgb_model, rf_model, voting_soft, bagging]\n",
    "model_names = ['LightGBM', 'XGBoost', 'RandomForest', 'VotingSoft', 'Bagging']\n",
    "\n",
    "# 각 모델의 교차 검증 수행\n",
    "for name, model in zip(model_names, models):\n",
    "    # 정확도 계산\n",
    "    accuracy_scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n",
    "    \n",
    "    # AUROC 계산\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_pred_proba = cross_val_predict(model, X, y, cv=kf, method='predict_proba')\n",
    "        auroc_score = roc_auc_score(y, y_pred_proba, multi_class='ovr')\n",
    "    else:\n",
    "        auroc_score = None\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  - 평균 정확도: {accuracy_scores.mean()} (+/- {accuracy_scores.std() * 2})\")\n",
    "    if auroc_score:\n",
    "        print(f\"  - AUROC: {auroc_score}\")\n",
    "    else:\n",
    "        print(\"  - AUROC: Not available\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 최종 모델 선택 및 전체 데이터로 학습 (AUROC 기준)\n",
    "best_model = models[np.argmax([\n",
    "    roc_auc_score(y, cross_val_predict(model, X, y, cv=kf, method='predict_proba'), multi_class='ovr')\n",
    "    if hasattr(model, \"predict_proba\") else 0 \n",
    "    for model in models\n",
    "])]\n",
    "\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# 테스트 데이터에 대한 예측\n",
    "X_test = test_df.drop([\"target\", \"ID\"], axis=1)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "if hasattr(best_model, \"predict_proba\"):\n",
    "    y_test_pred_proba = best_model.predict_proba(X_test)\n",
    "\n",
    "# 제출 파일 생성\n",
    "submission = submission.assign(target=y_test_pred)\n",
    "submission.to_csv(\"output_best_ensemble.csv\", index=False)\n",
    "\n",
    "print(f\"Best model: {type(best_model).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앙상블의 inference & save 과정."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('datawrg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "ab2bf47c83e23e79da7154310486cd6f2111092cec5daef28d72dd2b3b6f44d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
