{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 중에서 안깔린 라이브러리가 있을 수 있음\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import lightgbm as lgb\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../data/\"\n",
    "\n",
    "train = pd.read_csv(path+\"train.csv\").assign(_type=\"train\")\n",
    "test = pd.read_csv(path+\"test.csv\").assign(_type=\"test\")\n",
    "submission = pd.read_csv(path+\"test.csv\")\n",
    "df = pd.concat([train, test], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 원래 베이스라인 코드 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # HOURLY_ 로 시작하는 .csv 파일 이름을 file_names 에 할딩\n",
    "# file_names: List[str] = [\n",
    "#     f for f in os.listdir(data_path) if f.startswith(\"HOURLY_\") and f.endswith(\".csv\")\n",
    "# ]\n",
    "\n",
    "# # 파일명 : 데이터프레임으로 딕셔너리 형태로 저장\n",
    "# file_dict: Dict[str, pd.DataFrame] = {\n",
    "#     f.replace(\".csv\", \"\"): pd.read_csv(os.path.join(data_path, f)) for f in file_names\n",
    "# }\n",
    "\n",
    "# for _file_name, _df in tqdm(file_dict.items()):\n",
    "#     # 열 이름 중복 방지를 위해 {_file_name.lower()}_{col.lower()}로 변경, datetime 열을 ID로 변경\n",
    "#     _rename_rule = {\n",
    "#         col: f\"{_file_name.lower()}_{col.lower()}\" if col != \"datetime\" else \"ID\"\n",
    "#         for col in _df.columns\n",
    "#     }\n",
    "#     _df = _df.rename(_rename_rule, axis=1)\n",
    "#     df = df.merge(_df, on=\"ID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 밑의 한 셀이 바꾼 부분 (이 바꾼 부분을 전제로 뒤의 코드가 실행됨.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HOURLY_NETWORK-DATA_SUPPLY.csv',\n",
       " 'HOURLY_MARKET-DATA_COINBASE-PREMIUM-INDEX.csv',\n",
       " 'HOURLY_NETWORK-DATA_DIFFICULTY.csv',\n",
       " 'HOURLY_NETWORK-DATA_ADDRESSES-COUNT.csv',\n",
       " 'HOURLY_NETWORK-DATA_UTXO-COUNT.csv',\n",
       " 'HOURLY_NETWORK-DATA_VELOCITY.csv',\n",
       " 'HOURLY_MARKET-DATA_LIQUIDATIONS_ALL_EXCHANGE_ALL_SYMBOL.csv',\n",
       " 'HOURLY_NETWORK-DATA_FEES.csv',\n",
       " 'HOURLY_NETWORK-DATA_HASHRATE.csv',\n",
       " 'HOURLY_MARKET-DATA_FUNDING-RATES_ALL_EXCHANGE.csv',\n",
       " 'HOURLY_NETWORK-DATA_BLOCKREWARD.csv',\n",
       " 'HOURLY_NETWORK-DATA_FEES-TRANSACTION.csv',\n",
       " 'HOURLY_NETWORK-DATA_TRANSACTIONS-COUNT.csv',\n",
       " 'HOURLY_MARKET-DATA_OPEN-INTEREST_ALL_EXCHANGE_ALL_SYMBOL.csv',\n",
       " 'HOURLY_NETWORK-DATA_BLOCK-BYTES.csv',\n",
       " 'HOURLY_NETWORK-DATA_TOKENS-TRANSFERRED.csv',\n",
       " 'HOURLY_MARKET-DATA_PRICE-OHLCV_ALL_EXCHANGE_SPOT_BTC_USD.csv',\n",
       " 'HOURLY_NETWORK-DATA_BLOCK-INTERVAL.csv',\n",
       " 'HOURLY_NETWORK-DATA_BLOCK-COUNT.csv',\n",
       " 'HOURLY_MARKET-DATA_TAKER-BUY-SELL-STATS_ALL_EXCHANGE.csv']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 32.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# HOURLY_로 시작하는 모든 csv 파일 불러오기\n",
    "prefixes = [\"HOURLY_NETWORK-DATA_\",\n",
    "            \"HOURLY_MARKET-DATA_PRICE-OHLCV_ALL_EXCHANGE_SPOT_BTC_USD\",\n",
    "            \"HOURLY_MARKET-DATA_COINBASE-PREMIUM-INDEX\",\n",
    "            \"HOURLY_MARKET-DATA_FUNDING-RATES_ALL_EXCHANGE\",\n",
    "            \"HOURLY_MARKET-DATA_LIQUIDATIONS_ALL_EXCHANGE_ALL_SYMBOL\",\n",
    "            \"HOURLY_MARKET-DATA_OPEN-INTEREST_ALL_EXCHANGE_ALL_SYMBOL\",\n",
    "            \"HOURLY_MARKET-DATA_TAKER-BUY-SELL-STATS_ALL_EXCHANGE\"]\n",
    "\n",
    "file_names = [\n",
    "    f for f in os.listdir(path)\n",
    "    if any(f.startswith(prefix) for prefix in prefixes) and f.endswith(\".csv\")\n",
    "]\n",
    "display(file_names)\n",
    "file_dict: Dict[str, pd.DataFrame] = {\n",
    "    f.replace(\".csv\", \"\"): pd.read_csv(path+f) for f in file_names\n",
    "}\n",
    "\n",
    "\n",
    "for _file_name, _df in tqdm(file_dict.items()):\n",
    "    _rename_rule = {\n",
    "        col: f\"{col.lower()}\" if col != \"datetime\" else \"ID\"\n",
    "        for col in _df.columns\n",
    "    }\n",
    "    _df = _df.rename(_rename_rule, axis=1)\n",
    "    df = df.merge(_df, on=\"ID\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 결측치 (아거도 추가된 부분)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "volume                         2792\n",
       "close                          2792\n",
       "block_bytes                      54\n",
       "funding_rates                    32\n",
       "taker_buy_sell_ratio             31\n",
       "taker_sell_ratio                 31\n",
       "taker_buy_ratio                  31\n",
       "taker_sell_volume                31\n",
       "taker_buy_volume                 31\n",
       "transactions_count_mean          28\n",
       "supply_new                       28\n",
       "supply_total                     28\n",
       "fees_transaction_mean_usd        24\n",
       "fees_block_mean                  24\n",
       "fees_block_mean_usd              24\n",
       "fees_reward_percent              24\n",
       "difficulty                       24\n",
       "tokens_transferred_mean          24\n",
       "block_interval                   24\n",
       "fees_transaction_mean            24\n",
       "coinbase_premium_gap              6\n",
       "coinbase_premium_index            6\n",
       "transactions_count_total          4\n",
       "open_interest                     4\n",
       "block_count                       4\n",
       "utxo_count                        1\n",
       "addresses_count_sender            0\n",
       "addresses_count_receiver          0\n",
       "tokens_transferred_median         0\n",
       "tokens_transferred_total          0\n",
       "addresses_count_active            0\n",
       "long_liquidations_usd             0\n",
       "velocity_supply_total             0\n",
       "short_liquidations_usd            0\n",
       "fees_transaction_median_usd       0\n",
       "fees_transaction_median           0\n",
       "blockreward_usd                   0\n",
       "long_liquidations                 0\n",
       "short_liquidations                0\n",
       "hashrate                          0\n",
       "fees_total_usd                    0\n",
       "fees_total                        0\n",
       "blockreward                       0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "volume                         0.241690\n",
       "close                          0.241690\n",
       "block_bytes                    0.004675\n",
       "funding_rates                  0.002770\n",
       "taker_buy_sell_ratio           0.002684\n",
       "taker_sell_ratio               0.002684\n",
       "taker_buy_ratio                0.002684\n",
       "taker_sell_volume              0.002684\n",
       "taker_buy_volume               0.002684\n",
       "transactions_count_mean        0.002424\n",
       "supply_new                     0.002424\n",
       "supply_total                   0.002424\n",
       "fees_transaction_mean_usd      0.002078\n",
       "fees_block_mean                0.002078\n",
       "fees_block_mean_usd            0.002078\n",
       "fees_reward_percent            0.002078\n",
       "difficulty                     0.002078\n",
       "tokens_transferred_mean        0.002078\n",
       "block_interval                 0.002078\n",
       "fees_transaction_mean          0.002078\n",
       "coinbase_premium_gap           0.000519\n",
       "coinbase_premium_index         0.000519\n",
       "transactions_count_total       0.000346\n",
       "open_interest                  0.000346\n",
       "block_count                    0.000346\n",
       "utxo_count                     0.000087\n",
       "addresses_count_sender         0.000000\n",
       "addresses_count_receiver       0.000000\n",
       "tokens_transferred_median      0.000000\n",
       "tokens_transferred_total       0.000000\n",
       "addresses_count_active         0.000000\n",
       "long_liquidations_usd          0.000000\n",
       "velocity_supply_total          0.000000\n",
       "short_liquidations_usd         0.000000\n",
       "fees_transaction_median_usd    0.000000\n",
       "fees_transaction_median        0.000000\n",
       "blockreward_usd                0.000000\n",
       "long_liquidations              0.000000\n",
       "short_liquidations             0.000000\n",
       "hashrate                       0.000000\n",
       "fees_total_usd                 0.000000\n",
       "fees_total                     0.000000\n",
       "blockreward                    0.000000\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_new = df.drop([\"ID\", \"target\", \"_type\"], axis = 1)\n",
    "\n",
    "# 결측치 확인\n",
    "missing_data = df_new.isnull().sum()\n",
    "\n",
    "# 결측치 비율 확인\n",
    "missing_ratio = df_new.isnull().mean()\n",
    "\n",
    "\n",
    "display(missing_data.sort_values(ascending=False))\n",
    "display(missing_ratio.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supply_total                   0\n",
      "supply_new                     0\n",
      "coinbase_premium_gap           0\n",
      "coinbase_premium_index         0\n",
      "difficulty                     0\n",
      "addresses_count_active         0\n",
      "addresses_count_sender         0\n",
      "addresses_count_receiver       0\n",
      "utxo_count                     0\n",
      "velocity_supply_total          0\n",
      "long_liquidations              0\n",
      "short_liquidations             0\n",
      "long_liquidations_usd          0\n",
      "short_liquidations_usd         0\n",
      "fees_block_mean                0\n",
      "fees_block_mean_usd            0\n",
      "fees_total                     0\n",
      "fees_total_usd                 0\n",
      "fees_reward_percent            0\n",
      "hashrate                       0\n",
      "funding_rates                  0\n",
      "blockreward                    0\n",
      "blockreward_usd                0\n",
      "fees_transaction_mean          0\n",
      "fees_transaction_mean_usd      0\n",
      "fees_transaction_median        0\n",
      "fees_transaction_median_usd    0\n",
      "transactions_count_total       0\n",
      "transactions_count_mean        0\n",
      "open_interest                  0\n",
      "block_bytes                    0\n",
      "tokens_transferred_total       0\n",
      "tokens_transferred_mean        0\n",
      "tokens_transferred_median      0\n",
      "close                          0\n",
      "volume                         0\n",
      "block_interval                 0\n",
      "block_count                    0\n",
      "taker_buy_volume               0\n",
      "taker_sell_volume              0\n",
      "taker_buy_ratio                0\n",
      "taker_sell_ratio               0\n",
      "taker_buy_sell_ratio           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. 결측치 처리 - 앞/뒤 보간법 (forward/backward fill)\n",
    "\n",
    "# df_ffill = df_new.fillna(method='ffill')  # 앞의 값을 사용하여 결측치 채움\n",
    "# df_bfill = df_new.fillna(method='bfill')  # 뒤의 값을 사용하여 결측치 채움\n",
    "\n",
    "# 2. 결측치 처리 - 선형 보간법 (linear interpolation)\n",
    "df_interpolated = df_new.interpolate(method='linear')\n",
    "\n",
    "# 3. 결측치 처리 - 평균값으로 대체\n",
    "# df_mean_filled = df_new.fillna(eda_df.mean())\n",
    "\n",
    "df.update(df_interpolated)\n",
    "\n",
    "# 결측치 처리 후 확인\n",
    "print(df_interpolated.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 데이터를 로드할 때, (제공 받은)베이스라인 코드에서와 같이 거래소와 암호화폐 종류를 불러오도록 설정을 했다면\n",
    "\n",
    "아래 코드는 작동하지 않음\n",
    "\n",
    "거래소와 암호화폐 종류를 고려하지 않은 코드임을 유의.\n",
    "\n",
    "(수정을 해야할 것 같음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 독립변수에서 내가 사용할 독립변수 고르기\n",
    "\n",
    "\n",
    "# 전체 feature\n",
    "features = df.columns\n",
    "# 내가 쓸 feature\n",
    "fe = ['ID', 'target', '_type', 'funding_rates', 'open_interest',\n",
    "       'taker_buy_volume', 'taker_sell_volume', 'taker_buy_ratio',\n",
    "       'taker_sell_ratio', 'taker_buy_sell_ratio', 'difficulty',\n",
    "       'transactions_count_total', 'transactions_count_mean', 'block_count',\n",
    "       'fees_transaction_mean', 'fees_transaction_mean_usd',\n",
    "       'fees_transaction_median', 'fees_transaction_median_usd',\n",
    "       'fees_block_mean', 'fees_block_mean_usd', 'fees_total',\n",
    "       'fees_total_usd', 'fees_reward_percent', 'hashrate', 'utxo_count',\n",
    "       'tokens_transferred_total', 'tokens_transferred_mean',\n",
    "       'tokens_transferred_median', 'block_interval', 'velocity_supply_total',\n",
    "       'supply_total', 'supply_new', 'addresses_count_active',\n",
    "       'addresses_count_sender', 'addresses_count_receiver',\n",
    "       'blockreward', 'blockreward_usd', 'buy_sell_volume_ratio']\n",
    "\n",
    "# new_features에는 내가 사용하지 않을 feature들이 담기고 나중에 drop됨. 때문에, 이름이 좀 잘못된 듯\n",
    "new_features = [x for x in features if x not in fe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda 에서 파악한 차이와 차이의 음수, 양수 여부를 새로운 피쳐로 생성\n",
    "df = df.assign(\n",
    "    liquidation_diff=df[\"long_liquidations\"] - df[\"short_liquidations\"],\n",
    "    liquidation_usd_diff=df[\"long_liquidations_usd\"] - df[\"short_liquidations_usd\"],\n",
    "    volume_diff=df[\"taker_buy_volume\"] - df[\"taker_sell_volume\"],\n",
    "    liquidation_diffg=np.sign(df[\"long_liquidations\"] - df[\"short_liquidations\"]),\n",
    "    liquidation_usd_diffg=np.sign(df[\"long_liquidations_usd\"] - df[\"short_liquidations_usd\"]),\n",
    "    volume_diffg=np.sign(df[\"taker_buy_volume\"] - df[\"taker_sell_volume\"]),\n",
    "    buy_sell_volume_ratio=df[\"taker_buy_volume\"] / (df[\"taker_sell_volume\"] + 1),\n",
    ")\n",
    "\n",
    "# category, continuous 열을 따로 할당해둠\n",
    "category_cols = [\"liquidation_diffg\", \"liquidation_usd_diffg\", \"volume_diffg\"]\n",
    "conti_cols = [_ for _ in (df.columns)[3:] if _ not in [\"ID\", \"target\", \"_type\"]] + [\n",
    "    \"buy_sell_volume_ratio\",\n",
    "    \"liquidation_diff\",\n",
    "    \"liquidation_usd_diff\",\n",
    "    \"volume_diff\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_feature(\n",
    "    df: pd.DataFrame,\n",
    "    conti_cols: List[str],\n",
    "    intervals: List[int],\n",
    ") -> List[pd.Series]:\n",
    "    \"\"\"\n",
    "    연속형 변수의 shift feature 생성\n",
    "    Args:\n",
    "        df (pd.DataFrame)\n",
    "        conti_cols (List[str]): continuous colnames\n",
    "        intervals (List[int]): shifted intervals\n",
    "    Return:\n",
    "        List[pd.Series]\n",
    "    \"\"\"\n",
    "    df_shift_dict = [\n",
    "        df[conti_col].shift(interval).rename(f\"{conti_col}_{interval}\")\n",
    "        for conti_col in conti_cols\n",
    "        for interval in intervals\n",
    "    ]\n",
    "    return df_shift_dict\n",
    "\n",
    "# 지수 이동 평균\n",
    "def EMA(df, col, span=2):\n",
    "    return df[col].ewm(span=span).mean()\n",
    "\n",
    "# Wavlet Transform\n",
    "def WT(df, col, wavelet='db5', th=0.6):\n",
    "    signal = df[col].values\n",
    "    th = th*np.nanmax(signal)\n",
    "    coef = pywt.wavedec(signal, wavelet, mode=\"per\" )\n",
    "    coef[1:] = (pywt.threshold(i, value=th, mode=\"soft\" ) for i in coef[1:])\n",
    "    reconstructed = pywt.waverec(coef, wavelet, mode=\"per\" )\n",
    "    return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 🔥 피처엔지니어링 함수 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜 관련 생성\n",
    "def make_date_features(df: pd.DataFrame, date_column: str) -> Tuple[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    입력된 데이터프레임의 특정 날짜 열을 기준으로 연도, 월, 주, 요일, 시간을 추출하여 새로운 피처로 추가.\n",
    "    Args:\n",
    "        df (pd.DataFrame): 날짜 컬럼을 포함하는 데이터프레임.\n",
    "        date_column (str): 날짜 정보가 담긴 열 이름.\n",
    "    Returns:\n",
    "        pd.DataFrame: 날짜 관련 피처가 추가된 데이터프레임 반환.\n",
    "    \"\"\"\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "    df['year'] = df[date_column].dt.year  # 연도\n",
    "    df['month'] = df[date_column].dt.month  # 월 \n",
    "    df['week'] = df[date_column].dt.isocalendar().week  # 주\n",
    "    df['day_of_week'] = df[date_column].dt.dayofweek  # 요일 \n",
    "    df['hour'] = df[date_column].dt.hour  # 시간 \n",
    "    date_columns = [date_column, 'year', 'month', 'week', 'day_of_week', 'hour']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변동성, 차분 피처 생성 함수 \n",
    "def make_diff_change_feature(\n",
    "    df: pd.DataFrame, \n",
    "    columns_list: List[str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    주어진 변수들에 대한 변동성, 차분 피처를 생성하고, 결측값을 처리하는 함수.\n",
    "    Args:\n",
    "        df (pd.DataFrame): 데이터를 포함한 데이터프레임.\n",
    "        columns_list (List[str]): 변동성과 차분 피처를 추가하고자 하는 열 이름 목록.\n",
    "    Returns:\n",
    "        pd.DataFrame: 변동성과 차분 피처가 추가된 데이터프레임.\n",
    "    \"\"\"  \n",
    "    new_features = {}  # 새로 추가할 열을 저장할 딕셔너리\n",
    "    for col in columns_list:\n",
    "        if col in df.columns:\n",
    "            pct_change_col = f'{col}_pct_change'\n",
    "            diff_col = f'{col}_diff'\n",
    "            new_features[pct_change_col] = df[col].pct_change(fill_method=None)\n",
    "            new_features[diff_col] = df[col].diff()\n",
    "        else:\n",
    "            print(f\"Error: Cannot find '{col}' column\")\n",
    "    new_features_df = pd.DataFrame(new_features, index=df.index) # 새 피처를 데이터프레임으로\n",
    "    new_features_df = new_features_df.ffill().bfill()  # 결측값 처리 (고정: ffill, bfill)\n",
    "    df = pd.concat([df, new_features_df], axis=1)  # 기존 데이터프레임에 추가\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 롱/숏 비율\n",
    "def make_longshort_ratio_feature(\n",
    "    df: pd.DataFrame, \n",
    "    long_col: str, \n",
    "    short_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    롱/숏 비율을 계산하는 함수\n",
    "    Args:\n",
    "        df (pd.DataFrame): 입력 데이터프레임\n",
    "        long_col (str): 롱(liquidations) 데이터가 저장된 열 이름\n",
    "        short_col (str): 숏(liquidations) 데이터가 저장된 열 이름\n",
    "    Returns:\n",
    "        pd.DataFrame: 롱/숏 비율이 저장된 새로운 열이 추가된 데이터프레임\n",
    "    \"\"\"\n",
    "    df['long_short_ratio'] = df[long_col] / (df[short_col] + 1e-6)  # 분모가 0이 되는 것을 방지하기 위해 작은 값을 더함\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 청산/거래량 비율\n",
    "def make_liquidation_to_volume_ratio_feature(\n",
    "    df: pd.DataFrame, \n",
    "    long_col: str, \n",
    "    short_col: str, \n",
    "    buy_volume_col: str, \n",
    "    sell_volume_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    청산/거래량 비율을 계산하는 함수\n",
    "    Args:\n",
    "        df (pd.DataFrame): 입력 데이터프레임\n",
    "        long_col (str): 롱(liquidations) 데이터가 저장된 열 이름\n",
    "        short_col (str): 숏(liquidations) 데이터가 저장된 열 이름\n",
    "        buy_volume_col (str): 매수 거래량이 저장된 열 이름\n",
    "        sell_volume_col (str): 매도 거래량이 저장된 열 이름\n",
    "    Returns:\n",
    "        pd.DataFrame: 청산/거래량 비율을 저장한 새로운 열이 추가된 데이터프레임\n",
    "    \"\"\"\n",
    "    df['liquidation_to_volume_ratio'] = (\n",
    "        (df[long_col] + df[short_col]) / \n",
    "        (df[buy_volume_col] + df[sell_volume_col] + 1e-6)  # 분모가 0이 되는 것을 방지하기 위해 작은 값을 더함\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 청산된 USD 롱/숏 비율\n",
    "def make_liquidation_usd_ratio_feature(\n",
    "    df: pd.DataFrame, \n",
    "    long_usd_col: str, \n",
    "    short_usd_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    청산된 USD 롱/숏 비율을 계산하는 함수\n",
    "    Args:\n",
    "        df (pd.DataFrame): 입력 데이터프레임\n",
    "        long_usd_col (str): 롱(liquidations) USD 데이터가 저장된 열 이름\n",
    "        short_usd_col (str): 숏(liquidations) USD 데이터가 저장된 열 이름 \n",
    "    Returns:\n",
    "        pd.DataFrame: 청산된 USD 롱/숏 비율을 저장한 새로운 열이 추가된 데이터프레임\n",
    "    \"\"\"\n",
    "    df['liquidation_usd_ratio'] = df[long_usd_col] / (df[short_usd_col] + 1e-6)  # 분모가 0이 되는 것을 방지하기 위해 작은 값을 더함\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 펀딩 비율과 롱/숏 포지션 차이 곱\n",
    "def make_funding_rate_position_change_feature(\n",
    "    df: pd.DataFrame, \n",
    "    funding_rate_col: str, \n",
    "    long_liquidations_col: str, \n",
    "    short_liquidations_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    펀딩 비율과 롱/숏 포지션 차이를 곱하여 포지션 변화를 계산하는 함수\n",
    "    Args:\n",
    "        df (pd.DataFrame): 입력 데이터프레임\n",
    "        funding_rate_col (str): 펀딩 비율 데이터가 저장된 열 이름\n",
    "        long_liquidations_col (str): 롱 청산 데이터가 저장된 열 이름\n",
    "        short_liquidations_col (str): 숏 청산 데이터가 저장된 열 이름\n",
    "    Returns:\n",
    "        pd.DataFrame: 펀딩 비율에 따른 포지션 변화를 저장한 새로운 열이 추가된 데이터프레임\n",
    "    \"\"\"\n",
    "    df['funding_rate_position_change'] = df[funding_rate_col] * (\n",
    "        df[long_liquidations_col] - df[short_liquidations_col]\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프리미엄 갭과 프리미엄 인덱스의 차이\n",
    "def make_premium_diff_feature(\n",
    "    df: pd.DataFrame, \n",
    "    premium_gap_col: str, \n",
    "    premium_index_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    프리미엄 갭과 프리미엄 인덱스의 차이를 계산하는 함수\n",
    "    Args:\n",
    "        df (pd.DataFrame): 입력 데이터프레임\n",
    "        premium_gap_col (str): 프리미엄 갭 데이터가 저장된 열 이름\n",
    "        premium_index_col (str): 프리미엄 인덱스 데이터가 저장된 열 이름\n",
    "    Returns:\n",
    "        pd.DataFrame: 프리미엄 갭과 프리미엄 인덱스의 차이를 저장한 새로운 열이 추가된 데이터프레임\n",
    "    \"\"\"\n",
    "    df['premium_diff'] = df[premium_gap_col] - df[premium_index_col]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해시레이트와 난이도 간의 비율\n",
    "def make_hashrate_to_difficulty_feature(\n",
    "    df: pd.DataFrame, \n",
    "    hashrate_col: str, \n",
    "    difficulty_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    해시레이트와 난이도 간의 비율을 계산하는 함수\n",
    "    Args:\n",
    "        df (pd.DataFrame): 입력 데이터프레임\n",
    "        hashrate_col (str): 해시레이트 데이터가 저장된 열 이름\n",
    "        difficulty_col (str): 난이도 데이터가 저장된 열 이름\n",
    "    Returns:\n",
    "        pd.DataFrame: 해시레이트와 난이도 비율을 저장한 새로운 열이 추가된 데이터프레임\n",
    "    \"\"\"\n",
    "    df['hashrate_to_difficulty'] = df[hashrate_col] / (df[difficulty_col] + 1e-6)  # 분모가 0이 되는 것을 방지하기 위해 작은 값을 더함\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공급 변화율\n",
    "def make_supply_change_rate_feature(\n",
    "    df: pd.DataFrame, \n",
    "    new_supply_col: str, \n",
    "    total_supply_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    공급 변화율을 계산하는 함수\n",
    "    Args:\n",
    "        df (pd.DataFrame): 입력 데이터프레임\n",
    "        new_supply_col (str): 새로운 공급량 데이터가 저장된 열 이름\n",
    "        total_supply_col (str): 총 공급량 데이터가 저장된 열 이름\n",
    "    Returns:\n",
    "        pd.DataFrame: 공급 변화율을 저장한 새로운 열이 추가된 데이터프레임\n",
    "    \"\"\"\n",
    "    df['supply_change_rate'] = df[new_supply_col] / (df[total_supply_col] + 1e-6)  # 분모가 0이 되는 것을 방지하기 위해 작은 값을 더함\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 적용 \n",
    "df = make_date_features(df, 'ID') # 날짜 피처\n",
    "df = make_diff_change_feature(df, ['open_interest']) # 변동성, 차분 피처 \n",
    "df = make_longshort_ratio_feature(df, 'long_liquidations', 'short_liquidations') # 롱/숏 비율\n",
    "df = make_liquidation_to_volume_ratio_feature(df, 'long_liquidations', 'short_liquidations', 'taker_buy_volume', 'taker_sell_volume') # 청산/거래량 비율\n",
    "df = make_liquidation_usd_ratio_feature(df, 'long_liquidations_usd', 'short_liquidations_usd') # 청산된 USD 롱/숏 비율\n",
    "df = make_funding_rate_position_change_feature(df, 'funding_rates', 'long_liquidations_usd', 'short_liquidations_usd') # 펀딩 비율과 롱/숏 포지션 차이 곱\n",
    "df = make_premium_diff_feature(df, 'coinbase_premium_gap', 'coinbase_premium_index') # 프리미엄 갭과 프리미엄 인덱스의 차이\n",
    "df = make_hashrate_to_difficulty_feature(df, 'hashrate', 'difficulty') # 해시레이트와 난이도 간의 비율\n",
    "df = make_supply_change_rate_feature(df, 'supply_new', 'supply_total') # 공급 변화율\n",
    "\n",
    "conti_cols = conti_cols + ['open_interest_pct_change', 'open_interest_diff', 'long_short_ratio', 'liquidation_to_volume_ratio', 'liquidation_usd_ratio', 'funding_rate_position_change', 'premium_diff', 'hashrate_to_difficulty', 'supply_change_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n",
      "/tmp/ipykernel_777029/2084871417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c+\"WT\"] = WT(df, c)\n"
     ]
    }
   ],
   "source": [
    "# 위의 함수들을 실행한다.\n",
    "\n",
    "# 모든 수치형 컬럼에 대한 지수이동평균 계산해서 새로운 열로 할당\n",
    "for c in conti_cols:\n",
    "    df[c+\"_moving_avg_7\"] = EMA(df, c, 7)\n",
    "\n",
    "# 모든 수치형 컬럼에 대한 Wavelet transform을 계산해서 새로운 열로 할당\n",
    "for c in conti_cols:\n",
    "    df[c+\"WT\"] = WT(df, c)\n",
    "\n",
    "# 최대 24시간의 shift 피쳐를 계산\n",
    "shift_list = shift_feature(\n",
    "    df=df, conti_cols=conti_cols, intervals=[_ for _ in range(1, 24)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.concat(shift_list, axis=1)], axis=1)\n",
    "\n",
    "_target = df[\"target\"]\n",
    "df = df.ffill().fillna(-999).assign(target = _target)\n",
    "df = df.loc[:, ~df.columns.duplicated()]\n",
    "df = df.drop(columns=new_features)\n",
    "\n",
    "train_df = df.loc[df[\"_type\"]==\"train\"].drop(columns=[\"_type\"])\n",
    "test_df = df.loc[df[\"_type\"]==\"test\"].drop(columns=[\"_type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lightGBM 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM - acc: 0.4366438356164384, auroc: 0.6439185931199092\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# train_test_split 으로 valid set, train set 분리\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    train_df.drop([\"target\", \"ID\"], axis = 1), \n",
    "    train_df[\"target\"].astype(int), \n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# LightGBM 모델 생성 및 학습\n",
    "lgb_model = LGBMClassifier(\n",
    "    boosting_type=\"gbdt\",\n",
    "    objective=\"multiclass\",\n",
    "    num_class=4,\n",
    "    num_leaves=50,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=30,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "lgb_model.fit(\n",
    "    x_train, \n",
    "    y_train,\n",
    "    eval_set=[(x_valid, y_valid)],\n",
    "    eval_metric='multi_logloss'\n",
    ")\n",
    "\n",
    "# 예측 확률\n",
    "y_valid_pred_proba = lgb_model.predict_proba(x_valid)\n",
    "\n",
    "# 클래스 레이블 예측\n",
    "y_valid_pred_class = lgb_model.predict(x_valid)\n",
    "\n",
    "# 성능 평가\n",
    "accuracy_lgb = accuracy_score(y_valid, y_valid_pred_class)\n",
    "auroc_lgb = roc_auc_score(y_valid, y_valid_pred_proba, multi_class=\"ovr\")\n",
    "\n",
    "print(f\"LightGBM - acc: {accuracy_lgb}, auroc: {auroc_lgb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost - acc: 0.4440639269406393, auroc: 0.648953293189703\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# train_test_split 으로 valid set, train set 분리\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    train_df.drop([\"target\", \"ID\"], axis = 1), \n",
    "    train_df[\"target\"].astype(int), \n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 4,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 100,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# XGBoost 모델 학습 및 성능 체크\n",
    "xgb_model = xgb.XGBClassifier(**xgb_params)\n",
    "xgb_model.fit(x_train, y_train)\n",
    "\n",
    "# XGBoost로 예측\n",
    "y_valid_pred_xgb = xgb_model.predict_proba(x_valid)\n",
    "y_valid_pred_class_xgb = xgb_model.predict(x_valid)\n",
    "\n",
    "# XGBoost 성능 체크\n",
    "accuracy_xgb = accuracy_score(y_valid, y_valid_pred_class_xgb)\n",
    "auroc_xgb = roc_auc_score(y_valid, y_valid_pred_xgb, multi_class=\"ovr\")\n",
    "\n",
    "print(f\"XGBoost - acc: {accuracy_xgb}, auroc: {auroc_xgb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensenble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble - acc: 0.4394977168949772, auroc: 0.6523183340251395\n"
     ]
    }
   ],
   "source": [
    "ensemble_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lgb', lgb_model),\n",
    "        ('xgb', xgb_model)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "ensemble_model.fit(x_train, y_train)\n",
    "\n",
    "y_valid_pred_ensemble = ensemble_model.predict_proba(x_valid)\n",
    "y_valid_pred_class_ensemble = ensemble_model.predict(x_valid)\n",
    "\n",
    "accuracy_ensemble = accuracy_score(y_valid, y_valid_pred_class_ensemble)\n",
    "auroc_ensemble = roc_auc_score(y_valid, y_valid_pred_ensemble, multi_class=\"ovr\")\n",
    "\n",
    "print(f\"Ensemble - acc: {accuracy_ensemble}, auroc: {auroc_ensemble}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;lgb&#x27;,\n",
       "                              LGBMClassifier(learning_rate=0.05,\n",
       "                                             n_estimators=30, num_class=4,\n",
       "                                             num_leaves=50,\n",
       "                                             objective=&#x27;multiclass&#x27;,\n",
       "                                             random_state=42, verbose=-1)),\n",
       "                             (&#x27;xgb&#x27;,\n",
       "                              XGBClassifier(base_score=None, booster=None,\n",
       "                                            callbacks=None,\n",
       "                                            colsample_bylevel=None,\n",
       "                                            colsample_bynode=None,\n",
       "                                            colsample_bytree=None, device=None,\n",
       "                                            early_stopping_rounds=None,\n",
       "                                            enable_categori...\n",
       "                                            grow_policy=None,\n",
       "                                            importance_type=None,\n",
       "                                            interaction_constraints=None,\n",
       "                                            learning_rate=0.05, max_bin=None,\n",
       "                                            max_cat_threshold=None,\n",
       "                                            max_cat_to_onehot=None,\n",
       "                                            max_delta_step=None, max_depth=6,\n",
       "                                            max_leaves=None,\n",
       "                                            min_child_weight=None, missing=nan,\n",
       "                                            monotone_constraints=None,\n",
       "                                            multi_strategy=None,\n",
       "                                            n_estimators=100, n_jobs=None,\n",
       "                                            num_class=4, num_parallel_tree=None, ...))],\n",
       "                 voting=&#x27;soft&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;lgb&#x27;,\n",
       "                              LGBMClassifier(learning_rate=0.05,\n",
       "                                             n_estimators=30, num_class=4,\n",
       "                                             num_leaves=50,\n",
       "                                             objective=&#x27;multiclass&#x27;,\n",
       "                                             random_state=42, verbose=-1)),\n",
       "                             (&#x27;xgb&#x27;,\n",
       "                              XGBClassifier(base_score=None, booster=None,\n",
       "                                            callbacks=None,\n",
       "                                            colsample_bylevel=None,\n",
       "                                            colsample_bynode=None,\n",
       "                                            colsample_bytree=None, device=None,\n",
       "                                            early_stopping_rounds=None,\n",
       "                                            enable_categori...\n",
       "                                            grow_policy=None,\n",
       "                                            importance_type=None,\n",
       "                                            interaction_constraints=None,\n",
       "                                            learning_rate=0.05, max_bin=None,\n",
       "                                            max_cat_threshold=None,\n",
       "                                            max_cat_to_onehot=None,\n",
       "                                            max_delta_step=None, max_depth=6,\n",
       "                                            max_leaves=None,\n",
       "                                            min_child_weight=None, missing=nan,\n",
       "                                            monotone_constraints=None,\n",
       "                                            multi_strategy=None,\n",
       "                                            n_estimators=100, n_jobs=None,\n",
       "                                            num_class=4, num_parallel_tree=None, ...))],\n",
       "                 voting=&#x27;soft&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lgb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(learning_rate=0.05, n_estimators=30, num_class=4, num_leaves=50,\n",
       "               objective=&#x27;multiclass&#x27;, random_state=42, verbose=-1)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>xgb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=100, n_jobs=None, num_class=4,\n",
       "              num_parallel_tree=None, ...)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingClassifier(estimators=[('lgb',\n",
       "                              LGBMClassifier(learning_rate=0.05,\n",
       "                                             n_estimators=30, num_class=4,\n",
       "                                             num_leaves=50,\n",
       "                                             objective='multiclass',\n",
       "                                             random_state=42, verbose=-1)),\n",
       "                             ('xgb',\n",
       "                              XGBClassifier(base_score=None, booster=None,\n",
       "                                            callbacks=None,\n",
       "                                            colsample_bylevel=None,\n",
       "                                            colsample_bynode=None,\n",
       "                                            colsample_bytree=None, device=None,\n",
       "                                            early_stopping_rounds=None,\n",
       "                                            enable_categori...\n",
       "                                            grow_policy=None,\n",
       "                                            importance_type=None,\n",
       "                                            interaction_constraints=None,\n",
       "                                            learning_rate=0.05, max_bin=None,\n",
       "                                            max_cat_threshold=None,\n",
       "                                            max_cat_to_onehot=None,\n",
       "                                            max_delta_step=None, max_depth=6,\n",
       "                                            max_leaves=None,\n",
       "                                            min_child_weight=None, missing=nan,\n",
       "                                            monotone_constraints=None,\n",
       "                                            multi_strategy=None,\n",
       "                                            n_estimators=100, n_jobs=None,\n",
       "                                            num_class=4, num_parallel_tree=None, ...))],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 성능 체크 후 전체 학습 데이터로 재학습\n",
    "x_train_full = train_df.drop([\"target\", \"ID\"], axis=1)\n",
    "y_train_full = train_df[\"target\"].astype(int)\n",
    "\n",
    "# LightGBM 전체 데이터 재학습\n",
    "lgb_model_full = LGBMClassifier(\n",
    "    boosting_type=\"gbdt\",\n",
    "    objective=\"multiclass\",\n",
    "    num_class=4,\n",
    "    num_leaves=50,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=30,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "lgb_model_full.fit(x_train_full, y_train_full)\n",
    "\n",
    "# XGBoost 전체 데이터 재학습\n",
    "xgb_model_full = xgb.XGBClassifier(\n",
    "    objective='multi:softprob',\n",
    "    num_class=4,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_model_full.fit(x_train_full, y_train_full)\n",
    "\n",
    "# 앙상블 모델 전체 데이터 재학습\n",
    "ensemble_model_full = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lgb', lgb_model_full),\n",
    "        ('xgb', xgb_model_full)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "ensemble_model_full.fit(x_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble - acc: 0.8807077625570776, auroc: 0.9943717564458966\n"
     ]
    }
   ],
   "source": [
    "y_valid_pred_ensemble = ensemble_model_full.predict_proba(x_valid)\n",
    "y_valid_pred_class_ensemble = ensemble_model_full.predict(x_valid)\n",
    "\n",
    "\n",
    "accuracy_ensemble = accuracy_score(y_valid, y_valid_pred_class_ensemble)\n",
    "auroc_ensemble = roc_auc_score(y_valid, y_valid_pred_ensemble, multi_class=\"ovr\")\n",
    "\n",
    "print(f\"Ensemble - acc: {accuracy_ensemble}, auroc: {auroc_ensemble}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앙상블의 inference & save 과정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터에 대한 최종 예측\n",
    "x_test = test_df.drop([\"target\", \"ID\"], axis=1)\n",
    "\n",
    "y_test_pred_proba_ensemble = ensemble_model_full.predict_proba(x_test)\n",
    "y_test_pred_class_ensemble = ensemble_model_full.predict(x_test)\n",
    "\n",
    "# 제출 파일 생성\n",
    "submission = submission.assign(target=y_test_pred_class_ensemble)\n",
    "submission.to_csv(\"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('datawrg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "ab2bf47c83e23e79da7154310486cd6f2111092cec5daef28d72dd2b3b6f44d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
